{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pm4py\n",
    "import numpy as np\n",
    "#from scipy.stats import mannwhitneyu\n",
    "#import collections\n",
    "#from statistics import mean\n",
    "#from statistics import median\n",
    "from collections import defaultdict\n",
    "from Levenshtein import distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "841f7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 300/300 [00:00<00:00, 2587.44it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 300/300 [00:00<00:00, 2172.03it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 300/300 [00:00<00:00, 1495.30it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 300/300 [00:00<00:00, 1771.49it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 509.56it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 921.23it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 577.80it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 747.77it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 2806.03it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 1772.72it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 2173.92it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 150/150 [00:00<00:00, 725.66it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 600/600 [00:00<00:00, 1387.65it/s]\n",
      "parsing log, completed traces :: 100%|██████████| 600/600 [00:00<00:00, 1895.54it/s]\n"
     ]
    }
   ],
   "source": [
    "log1 = pm4py.read.read_xes(\"SynLogs\\Choice_Long_NEW.gz\")\n",
    "log1[\"case:concept:name\"] = log1[\"case:concept:name\"] + 'C1'\n",
    "log1['type'] = 'C1'\n",
    "\n",
    "log2 = pm4py.read.read_xes(\"SynLogs\\Choice_Short_NEW.gz\")\n",
    "log2[\"case:concept:name\"] = log2[\"case:concept:name\"] + 'C2'\n",
    "log2['type'] = 'C2'\n",
    "\n",
    "\n",
    "log7 = pm4py.read.read_xes(\"SynLogs\\Choice_Long_X_NEW.gz\")\n",
    "log7[\"case:concept:name\"] = log7[\"case:concept:name\"] + 'C3'\n",
    "log7['type'] = 'C3'\n",
    "\n",
    "log8 = pm4py.read.read_xes(\"SynLogs\\Choice_Short_X_NEW.gz\")\n",
    "log8[\"case:concept:name\"] = log8[\"case:concept:name\"] + 'C4'\n",
    "log8['type'] = 'C4'\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "log3 = pm4py.read.read_xes(\"SynLogs\\Loops_Long.xes\")\n",
    "log3[\"case:concept:name\"] = log3[\"case:concept:name\"] + 'L1'\n",
    "log3['type'] = 'L1'\n",
    "\n",
    "\n",
    "log4 = pm4py.read.read_xes(\"SynLogs\\Loops_Short.xes\")\n",
    "log4[\"case:concept:name\"] = log4[\"case:concept:name\"] + 'L2'\n",
    "log4['type'] = 'L2'\n",
    "\n",
    "\n",
    "log9 = pm4py.read.read_xes(\"SynLogs\\Loops_Long_X.xes\")\n",
    "log9[\"case:concept:name\"] = log9[\"case:concept:name\"] + 'L3'\n",
    "log9['type'] = 'L3'\n",
    "\n",
    "\n",
    "log10 = pm4py.read.read_xes(\"SynLogs\\Loops_Short_X.xes\")\n",
    "log10[\"case:concept:name\"] = log10[\"case:concept:name\"] + 'L4'\n",
    "log10['type'] = 'L4'\n",
    "\n",
    "###\n",
    "\n",
    "log5 = pm4py.read.read_xes(\"SynLogs\\Skips_Long.xes\")\n",
    "log5[\"case:concept:name\"] = log5[\"case:concept:name\"] + 'S1'\n",
    "log5['type'] = 'S1'\n",
    "\n",
    "\n",
    "log6 = pm4py.read.read_xes(\"SynLogs\\Skips_Short.xes\")\n",
    "log6[\"case:concept:name\"] = log6[\"case:concept:name\"] + 'S2'\n",
    "log6['type'] = 'S2'\n",
    "\n",
    "log11 = pm4py.read.read_xes(\"SynLogs\\Skips_Long_X.xes\")\n",
    "log11[\"case:concept:name\"] = log11[\"case:concept:name\"] + 'S3'\n",
    "log11['type'] = 'S3'\n",
    "\n",
    "\n",
    "log12 = pm4py.read.read_xes(\"SynLogs\\Skips_Short_X.xes\")\n",
    "log12[\"case:concept:name\"] = log12[\"case:concept:name\"] + 'S4'\n",
    "log12['type'] = 'S4'\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "log13 = pm4py.read.read_xes(\"SynLogs\\Choice_Skip_Short_Long_NEW.gz\")\n",
    "log13[\"case:concept:name\"] = log13[\"case:concept:name\"] + 'M1'\n",
    "log13['type'] = 'M1'\n",
    "\n",
    "\n",
    "log14 = pm4py.read.read_xes(\"SynLogs\\Choice_Skip_Short_Long_X_NEW.gz\")\n",
    "log14[\"case:concept:name\"] = log14[\"case:concept:name\"] + 'M2'\n",
    "log14['type'] = 'M2'\n",
    "\n",
    "\n",
    "log = pd.concat([log1, log2, log3, log4, log5, log6, log11, log12, log10, log9, log8, log7, log13, log14])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef48af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'tabName_element' column\n",
    "log['activity'] = le.fit_transform(log['concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d64e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create trace log\n",
    "logVar = log.groupby(['case:concept:name'])['activity'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9eb23b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logType = log.groupby(['case:concept:name'])['type'].apply(list).reset_index()\n",
    "logType['type'] = logType['type'].apply(lambda x: set(x))\n",
    "logType['type'] = logType['type'].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67c5761a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar0 = pd.merge(logVar, logType, on='case:concept:name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8099716",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M1    600\n",
       "M2    600\n",
       "C1    300\n",
       "C2    300\n",
       "C3    300\n",
       "C4    300\n",
       "L1    150\n",
       "L2    150\n",
       "L3    150\n",
       "L4    150\n",
       "S1    150\n",
       "S2    150\n",
       "S3    150\n",
       "S4    150\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar0['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2df654a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>activity</th>\n",
       "      <th>type</th>\n",
       "      <th>act_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case_0C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7]</td>\n",
       "      <td>C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>case_0C2</td>\n",
       "      <td>[0, 1, 5, 6, 7, 0, 4, 5, 6, 7]</td>\n",
       "      <td>C2</td>\n",
       "      <td>[0, 1, 5, 6, 7, 0, 4, 5, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case_0C3</td>\n",
       "      <td>[8, 9, 10, 15, 8, 13, 14, 15, 8, 11, 12, 15, 8...</td>\n",
       "      <td>C3</td>\n",
       "      <td>[8, 9, 10, 15, 8, 13, 14, 15, 8, 11, 12, 15, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case_0C4</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 14, 15]</td>\n",
       "      <td>C4</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 14, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>case_0L1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>L1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>case_9L1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>L1</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>case_9L2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "      <td>L2</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>case_9L3</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...</td>\n",
       "      <td>L3</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>case_9L4</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...</td>\n",
       "      <td>L4</td>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>case_9M1</td>\n",
       "      <td>[0, 1, 5, 6, 7, 0, 2, 5, 7, 0, 2, 5, 7, 0, 5, ...</td>\n",
       "      <td>M1</td>\n",
       "      <td>[0, 1, 5, 6, 7, 0, 2, 5, 7, 0, 2, 5, 7, 0, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case:concept:name                                           activity  \\\n",
       "0             case_0C1                           [0, 1, 2, 7, 0, 3, 4, 7]   \n",
       "1             case_0C2                     [0, 1, 5, 6, 7, 0, 4, 5, 6, 7]   \n",
       "2             case_0C3  [8, 9, 10, 15, 8, 13, 14, 15, 8, 11, 12, 15, 8...   \n",
       "3             case_0C4              [8, 9, 13, 14, 15, 8, 10, 13, 14, 15]   \n",
       "4             case_0L1  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...   \n",
       "...                ...                                                ...   \n",
       "1152          case_9L1  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...   \n",
       "1153          case_9L2  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...   \n",
       "1154          case_9L3  [8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...   \n",
       "1155          case_9L4  [8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...   \n",
       "1156          case_9M1  [0, 1, 5, 6, 7, 0, 2, 5, 7, 0, 2, 5, 7, 0, 5, ...   \n",
       "\n",
       "     type                                            act_str  \n",
       "0      C1                           [0, 1, 2, 7, 0, 3, 4, 7]  \n",
       "1      C2                     [0, 1, 5, 6, 7, 0, 4, 5, 6, 7]  \n",
       "2      C3  [8, 9, 10, 15, 8, 13, 14, 15, 8, 11, 12, 15, 8...  \n",
       "3      C4              [8, 9, 13, 14, 15, 8, 10, 13, 14, 15]  \n",
       "4      L1  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...  \n",
       "...   ...                                                ...  \n",
       "1152   L1  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...  \n",
       "1153   L2  [0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, ...  \n",
       "1154   L3  [8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...  \n",
       "1155   L4  [8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 1...  \n",
       "1156   M1  [0, 1, 5, 6, 7, 0, 2, 5, 7, 0, 2, 5, 7, 0, 5, ...  \n",
       "\n",
       "[1157 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Derive set of unique trace variants + count\n",
    "logVar0[\"act_str\"] = logVar0[\"activity\"].apply(lambda x: str(x))\n",
    "#logVar0['Count'] = logVar0.groupby(['act_str'])['activity'].transform('count')\n",
    "logVar0 = logVar0.drop_duplicates(subset=['act_str'], keep='first')\n",
    "logVar0 = logVar0.reset_index(drop=True)\n",
    "logVar0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38adad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1    132\n",
       "L2    124\n",
       "L3    123\n",
       "L4    121\n",
       "M1    105\n",
       "M2     88\n",
       "S4     79\n",
       "S2     75\n",
       "C2     63\n",
       "C4     60\n",
       "S3     59\n",
       "S1     51\n",
       "C1     42\n",
       "C3     35\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar0['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07247894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logVar0.loc[logVar0['type'] == 'C1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65eeb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=20\n",
    "\n",
    "logVar1 = logVar0.loc[logVar0['type'] == 'C1']\n",
    "logVar1 = logVar1.sample(n)\n",
    "logVar1['type'] = 'C'\n",
    "\n",
    "logVar7 = logVar0.loc[logVar0['type'] == 'C3']\n",
    "logVar7 = logVar7.sample(n)\n",
    "logVar7['type'] = 'C'\n",
    "\n",
    "\n",
    "\n",
    "logVar2 = logVar0.loc[logVar0['type'] == 'C2']\n",
    "logVar2 = logVar2.sample(n)\n",
    "logVar2['type'] = 'C'\n",
    "\n",
    "logVar8 = logVar0.loc[logVar0['type'] == 'C4']\n",
    "logVar8 = logVar8.sample(n)\n",
    "logVar8['type'] = 'C'\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "logVar3 = logVar0.loc[logVar0['type'] == 'L1']\n",
    "logVar3 = logVar3.sample(n)\n",
    "logVar3['type'] = 'L'\n",
    "\n",
    "logVar9 = logVar0.loc[logVar0['type'] == 'L3']\n",
    "logVar9 = logVar9.sample(n)\n",
    "logVar9['type'] = 'L'\n",
    "\n",
    "\n",
    "\n",
    "logVar4 = logVar0.loc[logVar0['type'] == 'L2']\n",
    "logVar4 = logVar4.sample(n)\n",
    "logVar4['type'] = 'L'\n",
    "\n",
    "logVar10 = logVar0.loc[logVar0['type'] == 'L4']\n",
    "logVar10 = logVar10.sample(n)\n",
    "logVar10['type'] = 'L'\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "logVar5 = logVar0.loc[logVar0['type'] == 'S1']\n",
    "logVar5 = logVar5.sample(n)\n",
    "logVar5['type'] = 'S'\n",
    "\n",
    "logVar11 = logVar0.loc[logVar0['type'] == 'S3']\n",
    "logVar11 = logVar11.sample(n)\n",
    "logVar11['type'] = 'S'\n",
    "\n",
    "\n",
    "\n",
    "logVar6 = logVar0.loc[logVar0['type'] == 'S2']\n",
    "logVar6 = logVar6.sample(n)\n",
    "logVar6['type'] = 'S'\n",
    "\n",
    "logVar12 = logVar0.loc[logVar0['type'] == 'S4']\n",
    "logVar12 = logVar12.sample(n)\n",
    "logVar12['type'] = 'S'\n",
    "\n",
    "###\n",
    "\n",
    "logVar13 = logVar0.loc[logVar0['type'] == 'M1']\n",
    "logVar13 = logVar13.sample(40)\n",
    "logVar13['type'] = 'M'\n",
    "\n",
    "logVar14 = logVar0.loc[logVar0['type'] == 'M2']\n",
    "logVar14 = logVar14.sample(40)\n",
    "logVar14['type'] = 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df75adf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>activity</th>\n",
       "      <th>type</th>\n",
       "      <th>act_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566</td>\n",
       "      <td>case_223C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538</td>\n",
       "      <td>case_204C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>995</td>\n",
       "      <td>case_71C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>case_251C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508</td>\n",
       "      <td>case_187C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>559</td>\n",
       "      <td>case_219M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>454</td>\n",
       "      <td>case_152M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>344</td>\n",
       "      <td>case_135M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>478</td>\n",
       "      <td>case_163M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>774</td>\n",
       "      <td>case_417M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index case:concept:name  \\\n",
       "0      566        case_223C1   \n",
       "1      538        case_204C1   \n",
       "2      995         case_71C1   \n",
       "3      603        case_251C1   \n",
       "4      508        case_187C1   \n",
       "..     ...               ...   \n",
       "315    559        case_219M2   \n",
       "316    454        case_152M2   \n",
       "317    344        case_135M2   \n",
       "318    478        case_163M2   \n",
       "319    774        case_417M2   \n",
       "\n",
       "                                              activity type  \\\n",
       "0    [0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...    C   \n",
       "1    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "2    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "3     [0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]    C   \n",
       "4    [0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "..                                                 ...  ...   \n",
       "315  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...    M   \n",
       "316  [8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...    M   \n",
       "317  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...    M   \n",
       "318  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...    M   \n",
       "319  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...    M   \n",
       "\n",
       "                                               act_str  \n",
       "0    [0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...  \n",
       "1    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "2    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "3     [0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]  \n",
       "4    [0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "..                                                 ...  \n",
       "315  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...  \n",
       "316  [8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...  \n",
       "317  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...  \n",
       "318  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...  \n",
       "319  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...  \n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar = pd.concat([logVar1, logVar2, logVar3, logVar4, logVar5, logVar6, logVar11, logVar12, logVar10, logVar9, logVar8, logVar7, logVar13, logVar14]).reset_index()\n",
    "logVar\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "717c5345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    80\n",
       "L    80\n",
       "S    80\n",
       "M    80\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04b06c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar.to_csv('logVar_PLG_SynL4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62266631",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar = pd.read_csv('logVar_PLG_SynL4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ad2d53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>activity</th>\n",
       "      <th>type</th>\n",
       "      <th>act_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566</td>\n",
       "      <td>case_223C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538</td>\n",
       "      <td>case_204C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>995</td>\n",
       "      <td>case_71C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603</td>\n",
       "      <td>case_251C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>508</td>\n",
       "      <td>case_187C1</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "      <td>C</td>\n",
       "      <td>[0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>559</td>\n",
       "      <td>case_219M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>454</td>\n",
       "      <td>case_152M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>344</td>\n",
       "      <td>case_135M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>478</td>\n",
       "      <td>case_163M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>774</td>\n",
       "      <td>case_417M2</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...</td>\n",
       "      <td>M</td>\n",
       "      <td>[8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index case:concept:name  \\\n",
       "0      566        case_223C1   \n",
       "1      538        case_204C1   \n",
       "2      995         case_71C1   \n",
       "3      603        case_251C1   \n",
       "4      508        case_187C1   \n",
       "..     ...               ...   \n",
       "315    559        case_219M2   \n",
       "316    454        case_152M2   \n",
       "317    344        case_135M2   \n",
       "318    478        case_163M2   \n",
       "319    774        case_417M2   \n",
       "\n",
       "                                              activity type  \\\n",
       "0    [0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...    C   \n",
       "1    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "2    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "3     [0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]    C   \n",
       "4    [0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...    C   \n",
       "..                                                 ...  ...   \n",
       "315  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...    M   \n",
       "316  [8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...    M   \n",
       "317  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...    M   \n",
       "318  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...    M   \n",
       "319  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...    M   \n",
       "\n",
       "                                               act_str  \n",
       "0    [0, 1, 2, 7, 0, 5, 6, 7, 0, 5, 6, 7, 0, 5, 6, ...  \n",
       "1    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "2    [0, 1, 2, 7, 0, 5, 6, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "3     [0, 1, 2, 7, 0, 3, 4, 7, 0, 5, 6, 7, 0, 3, 4, 7]  \n",
       "4    [0, 1, 2, 7, 0, 3, 4, 7, 0, 3, 4, 7, 0, 5, 6, ...  \n",
       "..                                                 ...  \n",
       "315  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 10...  \n",
       "316  [8, 9, 13, 14, 15, 8, 10, 13, 15, 8, 10, 13, 1...  \n",
       "317  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...  \n",
       "318  [8, 9, 13, 14, 15, 8, 11, 12, 13, 15, 8, 13, 1...  \n",
       "319  [8, 9, 13, 14, 15, 8, 13, 15, 8, 13, 15, 8, 13...  \n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cac2a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "logVar[\"activity\"] = logVar[\"activity\"].apply(lambda x: ast.literal_eval(x))\n",
    "logVar[\"c:n_chr\"] = logVar[\"activity\"].apply(lambda x: [chr(int(i)) for i in x])\n",
    "logVar[\"strings\"] = logVar[\"c:n_chr\"].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1090460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar[\"length\"] = logVar[\"activity\"].apply(lambda x: len(x))\n",
    "logVar['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e6821bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar['length'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bf24c",
   "metadata": {},
   "source": [
    "## Ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52b2134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Create dictionary with true labels\n",
    "logVar['class'] = le.fit_transform(logVar['type'])\n",
    "labelDict1 = logVar['class'].to_dict()\n",
    "#labelDict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16dce5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"c:n_chr\"] = logVar[\"activity\"].apply(lambda x: [chr(i) for i in x])\n",
    "logVar[\"strings\"] = logVar[\"c:n_chr\"].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1b48b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_calc(features, distance):\n",
    "    n = len(features)\n",
    "    dist_matrix = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix[i,j] = distance(features[i], features[j])\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "    \n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97389b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(DistMatrix):\n",
    "    print('NN:   ' + str(NearestNeighbor(DistMatrix, labelDict1)))\n",
    "    print('P@10: ' + str(PrecisionAtK(DistMatrix, labelDict1, 10)))\n",
    "\n",
    "    triplet = BatchAllTtripletLoss()\n",
    "    print('Tri:  ' + str(triplet.forward(DistMatrix,labelDict1)))\n",
    "    print('Sil:  ' + str(Silhouette(DistMatrix, labelDict1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0863e5",
   "metadata": {},
   "source": [
    "## Evaluation based on Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8407d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 8,  9,  5, 4, 0]])\\n\\nlabelDict2 = {0:1,1:2,2:1,3:3,4:3}\\nNearestNeighbor(x, labelDict2)\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NearestNeighbor(matrix, labelDict):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        #delete 0 in array matrix[i] for distance between (i,i)\n",
    "        x = np.delete(matrix[i], i) \n",
    "        #identify min distance/value in array\n",
    "        y = min(x)\n",
    "        \n",
    "        #identify position of y AND select first position/pair appearing in the array in case there are muliple pairs with identical min distance\n",
    "        nearestNeighbor = np.where(x == y)[0] #problem if multiple positions??\n",
    "        nearestNeighbor = int(nearestNeighbor[0])\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "        \n",
    "        #for label comparison add +1 to dict position if position occurs after i (because of the deletion of 0 at the beginning)\n",
    "        if nearestNeighbor >= i:\n",
    "            if trueLabel == labelDict[nearestNeighbor + 1]:\n",
    "                clusterMeasuredCount[trueLabel] += 1         \n",
    "        else:\n",
    "            if trueLabel == labelDict[nearestNeighbor]:\n",
    "                clusterMeasuredCount[trueLabel] += 1\n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterMeasuredCount)\n",
    "    #print(clusterTrueCount)\n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 8,  9,  5, 4, 0]])\n",
    "\n",
    "labelDict2 = {0:1,1:2,2:1,3:3,4:3}\n",
    "NearestNeighbor(x, labelDict2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aae66",
   "metadata": {},
   "source": [
    "## Evaluation based on Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24a0d082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx3 = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\\n\\nlabelDict3 = {0:1,1:2,2:1,3:3,4:3}\\nPrecisionAtK(x3, labelDict3, 2)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    #return None  # Return None if the value is not found in the dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PrecisionAtK(matrix, labelDict, k):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "\n",
    "        \n",
    "        #delete 0 in array matrix[i] for distance between (i,i)   \n",
    "        x = np.delete(matrix[i], i)\n",
    "        \n",
    "        #get all k minimum values\n",
    "        nnValues = []\n",
    "        for l in range(k):\n",
    "            y = min(x)\n",
    "            nnValues.append(y)\n",
    "            x = np.delete(x, np.where(x == y)[0][0])\n",
    "        \n",
    "        \n",
    "        #get location of minimum values\n",
    "        z = np.delete(matrix[i], i)\n",
    "        \n",
    "        #create dict from array with {position:value}\n",
    "        my_dict = {}\n",
    "        for m in range(len(z)):\n",
    "            my_dict[m] = z[m]\n",
    "        \n",
    "        key_list = []\n",
    "        for n in nnValues:\n",
    "            key = get_key_by_value(my_dict, n)\n",
    "            key_list.append(key)\n",
    "            del my_dict[key]\n",
    "        \n",
    "        for o in range(k):\n",
    "            position = list(key_list)[o]\n",
    "\n",
    "            if position >= i:\n",
    "                if trueLabel == labelDict[position + 1]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            else:\n",
    "                if trueLabel == labelDict[position]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            #transform array to dict\n",
    "            #get key value from dict\n",
    "            #compare label\n",
    "            #remove key+value from dict\n",
    "\n",
    "            #Need exception in case y == 0 ??\n",
    "            \n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / k / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x3 = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\n",
    "\n",
    "labelDict3 = {0:1,1:2,2:1,3:3,4:3}\n",
    "PrecisionAtK(x3, labelDict3, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18183bf",
   "metadata": {},
   "source": [
    "## Triplet\n",
    "\n",
    "see: https://towardsdatascience.com/triplet-loss-advanced-intro-49a07b7d8905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b8138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "836bd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_mask(labels):\n",
    "    \n",
    "    # step 1 - get a mask for distinct indices\n",
    "    ###print('labels', labels)\n",
    "    # shape: (batch_size, batch_size)\n",
    "    indices_equal = torch.eye(labels.size()[0], dtype=torch.bool, device=labels.device)\n",
    "    ###print('equal', indices_equal)\n",
    "    indices_not_equal = torch.logical_not(indices_equal)\n",
    "    ###print('not_equal', indices_not_equal)\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_not_equal_j = indices_not_equal.unsqueeze(2)\n",
    "    ###print('i_not_j - unsqueeze2', i_not_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_not_equal_k = indices_not_equal.unsqueeze(1)\n",
    "    ###print('i_not_k - unsqueeze1', i_not_equal_k)\n",
    "    # shape: (1, batch_size, batch_size)\n",
    "    j_not_equal_k = indices_not_equal.unsqueeze(0)\n",
    "    ###print('j_not_k - unsqueeze0', i_not_equal_k)\n",
    "    # Shape: (batch_size, batch_size, batch_size)\n",
    "    distinct_indices = torch.logical_and(torch.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "    ###print('distinct!!!!', distinct_indices)\n",
    "\n",
    "    # step 2 - get a mask for valid anchor-positive-negative triplets\n",
    "    # shape: (batch_size, batch_size)\n",
    "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    ###print('labels_equal', labels_equal)\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_equal_j = labels_equal.unsqueeze(2)\n",
    "    ###print('i_equal_j', i_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_equal_k = labels_equal.unsqueeze(1)\n",
    "    ###print('i_equal_k', i_equal_k)\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    valid_indices = torch.logical_and(i_equal_j, torch.logical_not(i_equal_k))\n",
    "    ###print('valid_indices!!!', valid_indices)\n",
    "    \n",
    "\n",
    "    # step 3 - combine two masks\n",
    "    mask = torch.logical_and(distinct_indices, valid_indices)\n",
    "    ###print('mask!!', mask)\n",
    "    return mask\n",
    "\n",
    "    \"\"\"compute a mask for valid triplets\n",
    "    Args:\n",
    "        labels: Batch of integer labels. shape: (batch_size,)\n",
    "    Returns:\n",
    "        Mask tensor to indicate which triplets are actually valid. Shape: (batch_size, batch_size, batch_size)\n",
    "        A triplet is valid if:\n",
    "        `labels[i] == labels[j] and labels[i] != labels[k]`\n",
    "        and `i`, `j`, `k` are different.\n",
    "    \"\"\"\n",
    "    \n",
    "class custom_activation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_activation, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchAllTtripletLoss(nn.Module):\n",
    "  \"\"\"Uses all valid triplets to compute Triplet loss\n",
    "\n",
    "  Args:\n",
    "    margin: Margin value in the Triplet Loss equation\n",
    "  \"\"\"\n",
    "  def __init__(self, margin=0): #default margin = 0\n",
    "    super().__init__()\n",
    "    self.margin = margin\n",
    "    self.relu = nn.ReLU() #new\n",
    "    self.custom = custom_activation()\n",
    "    \n",
    "  def forward(self, distance_matrix, labels):\n",
    "    \"\"\"computes loss value.\n",
    "\n",
    "    Args:\n",
    "      embeddings: Batch of embeddings, e.g., output of the encoder. shape: (batch_size, embedding_dim)\n",
    "      labels: Batch of integer labels associated with embeddings. shape: (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "      Scalar loss value.\n",
    "    \"\"\"\n",
    "    # step 1 - convert to tensor format\n",
    "    distance_matrix = torch.tensor(distance_matrix)\n",
    "    labels = torch.tensor(list(labels.values()))\n",
    "\n",
    "\n",
    "    # step 2 - compute loss values for all triplets by applying broadcasting to distance matrix\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    anchor_positive_dists = distance_matrix.unsqueeze(2)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    anchor_negative_dists = distance_matrix.unsqueeze(1)\n",
    "    # get loss values for all possible n^3 triplets\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    triplet_loss = anchor_negative_dists - anchor_positive_dists + self.margin\n",
    "    ###print('tl0',triplet_loss)\n",
    "\n",
    "    # step 3 - filter out invalid or easy triplets by setting their loss values to 0\n",
    "\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    mask = get_triplet_mask(labels)\n",
    "    ###print('mask', mask)\n",
    "    triplet_loss *= mask\n",
    "    ###print(triplet_loss)\n",
    "    ###print('tl1:', triplet_loss)\n",
    "    # easy triplets have negative loss values\n",
    "    \n",
    "    triplet_loss = self.custom(triplet_loss)\n",
    "    ###print(triplet_loss)\n",
    "    #triplet_loss = F.relu(triplet_loss)\n",
    "\n",
    "    # step 4 - compute scalar loss value by averaging positive losses\n",
    "    \n",
    "    triLossNonZero = (triplet_loss != 0).nonzero(as_tuple=True)\n",
    "    labelTorchUnique = torch.unique(labels, return_counts=True)\n",
    "    \n",
    "    nonZero = len(triLossNonZero[0])\n",
    "    triLossSum = []\n",
    "    for i in range(nonZero):\n",
    "        #Identify L_a --> In Class\n",
    "        t1 = triLossNonZero[0][i]\n",
    "        labelIn = labels[t1]\n",
    "        positionIn = int((labelTorchUnique[0] == labelIn).nonzero(as_tuple=False))\n",
    "        countIn = labelTorchUnique[1][positionIn]\n",
    "\n",
    "        #Identify L_b --> Out Class\n",
    "        t3 = triLossNonZero[2][i]\n",
    "        labelOut = labels[t3]\n",
    "        positionOut = int((labelTorchUnique[0] == labelOut).nonzero(as_tuple=False))\n",
    "        countOut = labelTorchUnique[1][positionOut]\n",
    "\n",
    "        #Calculate loss\n",
    "        value = (1/countIn)*(1/countIn)*(1/countOut)  \n",
    "        ###print(countIn)\n",
    "        ###print(countOut)\n",
    "        triLossSum.append(value)\n",
    "    \n",
    "    #finally divide by |A|^2-|A|\n",
    "    A = len(labelTorchUnique[0])  \n",
    "    lossValue = sum(triLossSum) / (A*A-A)\n",
    "        \n",
    "    #OLD\n",
    "    #E_triplet = (1 / (A^2 - A)) *\n",
    "    #num_positive_losses = (triplet_loss > eps).float().sum()\n",
    "    #print(num_positive_losses)\n",
    "    #print(triplet_loss.sum())\n",
    "    #triplet_loss = triplet_loss.sum() / (num_positive_losses + eps)\n",
    "    \n",
    "\n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12a92e",
   "metadata": {},
   "source": [
    "## Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26715870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def Silhouette(distMatrix, labelDict):\n",
    "    labelDictList = list(labelDict.values())\n",
    "    return metrics.silhouette_score(distMatrix, labelDictList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f908a1",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3b767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.95\n",
      "P@10: 0.8362499999999999\n",
      "Tri:  tensor(0.5297)\n",
      "Sil:  -0.07760801031320698\n"
     ]
    }
   ],
   "source": [
    "#Levenshtein Distance\n",
    "#from Levenshtein import distance\n",
    "\n",
    "lev_dis = matrix_calc(logVar[\"strings\"],distance)\n",
    "results(lev_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d769e7",
   "metadata": {},
   "source": [
    "### Normalized Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ecb15751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.953125\n",
      "P@10: 0.86125\n",
      "Tri:  tensor(0.4315)\n",
      "Sil:  0.04582165645710614\n"
     ]
    }
   ],
   "source": [
    "List = logVar[\"strings\"]\n",
    "\n",
    "n = len(List)\n",
    "dist_matrix = np.zeros((n,n))    # initialize distance matrix to a square of zeros\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        dist_matrix[i,j] = distance(List[i], List[j]) / max(len(List[i]),len(List[j]))\n",
    "        dist_matrix[j,i] = dist_matrix[i,j]       # for the symmetric part, no computation\n",
    "\n",
    "lev_dis_norm = dist_matrix\n",
    "results(lev_dis_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7dd7ae",
   "metadata": {},
   "source": [
    "### Cosine based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbe30ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 1-gram\n",
    "\n",
    "def createVector(charList):\n",
    "    #dtype = [('structure', 'S10'), ('relfrequ', float)]\n",
    "    arrayList = np.array(charList)\n",
    "    unique, counts = np.unique(arrayList, return_counts=True)\n",
    "    #calculate relative frequency\n",
    "    relFrequList = np.array((unique, counts)).T\n",
    "    uniqueList = list(unique)\n",
    "    return relFrequList[relFrequList[:, 0].argsort()]\n",
    "    #check completeness\n",
    "    #if 'tree' not in uniqueList:\n",
    "        #relFrequList = np.append(relFrequList, np.array([['tree', 0]]), axis=0)\n",
    "        #print(relFrequList)\n",
    "\n",
    "        \n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"1-gram\"] = logVar[\"c:n_chr\"].apply(lambda x: createVector(tuple(x)))\n",
    "#logVar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2556cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignArrays(array1, array2):\n",
    "    commonSet = set(array1[:,0]).union(array2[:,0])\n",
    "    \n",
    "    for i in commonSet:\n",
    "        if i not in array1[:,0]:\n",
    "            array1 = np.append(array1, np.array([[i, '0']]), axis=0)\n",
    "        if i not in array2[:,0]:\n",
    "            array2 = np.append(array2, np.array([[i, '0']]), axis=0)\n",
    "    return array1[array1[:, 0].argsort()], array2[array2[:, 0].argsort()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2a7a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cosineDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(int)\n",
    "    b = Vector2[:,1].astype(int)\n",
    "    dist_matrix = distance.cosine(a, b)\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c740178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9875\n",
      "P@10: 0.9321875\n",
      "Tri:  tensor(0.4453)\n",
      "Sil:  0.016480904227800533\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 1-gram\n",
    "\n",
    "cos1_dis = matrix_calc(logVar[\"1-gram\"],cosineDist)\n",
    "results(cos1_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaed8c6",
   "metadata": {},
   "source": [
    "### Cosine based on 2-gram and 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58972a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data format from string to list of unique characters\n",
    "#logVar[\"charList\"] = logVar[\"trace_variant\"].apply(lambda x: list(x))\n",
    "#logVar\n",
    "\n",
    "def df_list(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList)):\n",
    "        new = ''.join(extList[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"dfList\"] = logVar[\"c:n_chr\"].apply(lambda x: df_list(x))\n",
    "#logVar\n",
    "\n",
    "logVar[\"2-gram\"] = logVar[\"dfList\"].apply(lambda x: createVector(x))\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35b052a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.996875\n",
      "P@10: 0.9674999999999999\n",
      "Tri:  tensor(0.4577)\n",
      "Sil:  0.07539687748054147\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 2-gram\n",
    "\n",
    "cos2_dis = matrix_calc(logVar[\"2-gram\"],cosineDist)\n",
    "results(cos2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c760733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3-gram\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"charList\"] = logVar[\"c:n_chr\"].apply(lambda x: list(x))\n",
    "\n",
    "def df_list2(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList) - 1):\n",
    "        new = ''.join(extList[i:i+3])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bb4a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"dfList2\"] = logVar[\"charList\"].apply(lambda x: df_list2(x))\n",
    "logVar[\"3-gram\"] = logVar[\"dfList2\"].apply(lambda x: createVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc2474ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9634375\n",
      "Tri:  tensor(0.4567)\n",
      "Sil:  0.11031656900750404\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 3-gram\n",
    "\n",
    "cos3_dis = matrix_calc(logVar[\"3-gram\"],cosineDist)\n",
    "results(cos3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e75dd41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9693750000000001\n",
      "Tri:  tensor(0.4555)\n",
      "Sil:  0.0442347156624748\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate1 = cos1_dis + cos2_dis\n",
    "results(aggregate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "79dc2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9693750000000001\n",
      "Tri:  tensor(0.4572)\n",
      "Sil:  0.06380013397841348\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate2 = cos1_dis + cos2_dis + cos3_dis\n",
    "results(aggregate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b870cd",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea7fc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "# see https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "\n",
    "def euclidDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(float)\n",
    "    b = Vector2[:,1].astype(float)\n",
    "    euclidean_dist = np.linalg.norm(a-b)\n",
    "    return euclidean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50a2aab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.940625\n",
      "P@10: 0.8240624999999999\n",
      "Tri:  tensor(0.5591)\n",
      "Sil:  -0.0523146184777661\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 1-gram\n",
    "\n",
    "euc1_dis = matrix_calc(logVar[\"1-gram\"],euclidDist)\n",
    "results(euc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d70b36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9812500000000001\n",
      "P@10: 0.9268749999999999\n",
      "Tri:  tensor(0.5862)\n",
      "Sil:  -0.0254352790756025\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 2-gram\n",
    "\n",
    "euc2_dis = matrix_calc(logVar[\"2-gram\"],euclidDist)\n",
    "results(euc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b2deb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9875\n",
      "P@10: 0.9384375\n",
      "Tri:  tensor(0.6024)\n",
      "Sil:  -0.013031277433513955\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 3-gram\n",
    "\n",
    "euc3_dis = matrix_calc(logVar[\"3-gram\"],euclidDist)\n",
    "results(euc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "932b65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.978125\n",
      "P@10: 0.8984375\n",
      "Tri:  tensor(0.5750)\n",
      "Sil:  -0.044013297517589554\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_euc = euc1_dis + euc2_dis\n",
    "results(agg_euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efaef19c",
   "metadata": {},
   "source": [
    "### Jaccard based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "feb39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance based on activity type\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1, s2 = set(list1), set(list2)\n",
    "    return 1 - len(s1 & s2) / len(s1 | s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ba79bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.315625\n",
      "P@10: 0.37875\n",
      "Tri:  tensor(0.3114)\n",
      "Sil:  -0.013960385636547851\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 1-gram\n",
    "Jacc1_dis = matrix_calc(logVar[\"charList\"],jaccard_similarity)\n",
    "results(Jacc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f27102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.996875\n",
      "P@10: 0.9953125\n",
      "Tri:  tensor(0.4374)\n",
      "Sil:  0.051203995353748734\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 2-gram\n",
    "Jacc2_dis = matrix_calc(logVar[\"dfList\"],jaccard_similarity)\n",
    "results(Jacc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97242b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.990625\n",
      "Tri:  tensor(0.4448)\n",
      "Sil:  0.11786637224966667\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 3-gram\n",
    "Jacc3_dis = matrix_calc(logVar[\"dfList2\"],jaccard_similarity)\n",
    "results(Jacc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d616d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.996875\n",
      "P@10: 0.9665625\n",
      "Tri:  tensor(0.4286)\n",
      "Sil:  0.014146771897744482\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_jacc1 = Jacc1_dis + Jacc2_dis\n",
    "results(agg_jacc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "850894ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.996875\n",
      "P@10: 0.989375\n",
      "Tri:  tensor(0.4410)\n",
      "Sil:  0.042124327982999386\n"
     ]
    }
   ],
   "source": [
    "agg_jacc2 = Jacc1_dis + Jacc2_dis + Jacc3_dis\n",
    "results(agg_jacc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe6191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca8982bc",
   "metadata": {},
   "source": [
    "## Graph based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "553dcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now consider edge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "26f94a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. transfer strings (string numbers) to integers --> HERE we need a new encoding!\n",
    "\n",
    "def intEncoder(character_List):\n",
    "    return [np.where(np.array(list(dict.fromkeys(character_List)))==e)[0][0]for e in character_List]\n",
    "\n",
    "logVar[\"intList\"] = logVar[\"activity\"].apply(lambda x: intEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26a1b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. transfer intList to int_tupleList\n",
    "\n",
    "#Create tuple lists\n",
    "def tuple_list(list_of_encodedActivities):\n",
    "    #list.insert(0, '*')\n",
    "    #list.append('*')\n",
    "    list_new = []\n",
    "    last_element = list_of_encodedActivities[-1]\n",
    "    for i in range(len(list_of_encodedActivities)):\n",
    "        new = tuple(list_of_encodedActivities[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    if list_of_encodedActivities.count(last_element) == 1: #check wether last activity in trace has some adjancency relation\n",
    "        list_new.append((last_element,)) ### NOT Correct\n",
    "    return list_new\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#tuple_list(q)\n",
    "\n",
    "logVar[\"int_tupleList\"] = logVar[\"intList\"].apply(lambda x: tuple_list(x))\n",
    "#logVar[\"int_tupleList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4db1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generate Adjacency List\n",
    "\n",
    "def adj_list(list_of_tuples):\n",
    "    adj_list_new = {}\n",
    "    try:\n",
    "        for node1, node2 in list_of_tuples:\n",
    "            #print(node1, node2)\n",
    "            if node1 not in adj_list_new:\n",
    "                newlist = []\n",
    "                newlist.append(node2)\n",
    "                adj_list_new[node1] = newlist\n",
    "                #print(adj_list3)\n",
    "        \n",
    "            else:\n",
    "                if node2 not in adj_list_new[node1]:\n",
    "                    #mylist.extend(adj_list3[node1])\n",
    "                    adj_list_new[node1].append(node2)\n",
    "                    #print(adj_list3)\n",
    "                    #adj_list3[node1] = mylist\n",
    "    \n",
    "    #in case activity has no adjacent activity - only possible for last activity --> tuple: (lastAct,)\n",
    "    except ValueError as ve:\n",
    "        lastValue = list_of_tuples[-1][0] \n",
    "        adj_list_new[lastValue] = list()\n",
    "    return list(adj_list_new.values())\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#l = tuple_list(q)\n",
    "#adj_list(l)\n",
    "\n",
    "logVar[\"int_adjList\"] = logVar[\"int_tupleList\"].apply(lambda x: adj_list(x))\n",
    "#logVar[\"int_adjList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5433d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now considering length\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_4(graph, start, end):\n",
    "    \n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    #print(start, end)\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        #print(queue)\n",
    "        node, distance = queue.popleft()\n",
    "        #if not node:\n",
    "            #print(start, end, queue)\n",
    "            #print(\"GRAPH LIST\", graph)\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end:\n",
    "            return distance \n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "        \n",
    "#x = {0: [0, 1], 1: [2, 1, 0, int], 2:[2], [3: [1, 5, 3, 7], 4: [3], 5: [6, 5], 6: [1, 7], 7: [8, 9, 7], 8: [5, 8, 10], 9: [3]}\n",
    "#y = [[0, 1, 5], [1, 2], [3, 4], [4, 2], [5, 0], [3, 6], []]\n",
    "#bfs_4(y, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01800b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "def reverse_graph(graph):\n",
    "    reversed_graph = defaultdict(list)\n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            reversed_graph[neighbor].append(node)\n",
    "    return reversed_graph\n",
    "\n",
    "\n",
    "def bfs_5(graph, start, end):\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    visited = {}\n",
    "    while queue:\n",
    "        node, distance = queue.popleft()\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end: # maybe quicker if adjacent directly checked\n",
    "            return visited\n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "            if adjacent not in visited:\n",
    "                visited.update({adjacent:distance})\n",
    "\n",
    "            \n",
    "def common_ancestors(graph, node1, node2): \n",
    "    #remove cross type edge between node1 and node2\n",
    "    graph[node1].remove(node2) #maybe issue, if graph is changed and not copy of graph?\n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    graphReverse = reverse_graph(graph)\n",
    "    setNode1 = bfs_5(graphReverse, node1, 0)\n",
    "    setNode2 = bfs_5(graphReverse, node2, 0)\n",
    "    if next((a for a in list(setNode1) if a in list(setNode2)), None) == None:\n",
    "        firstCommonAnces = next((a for a in list(setNode2) if a in list(setNode1)), None)\n",
    "    else:\n",
    "        firstCommonAnces = next((a for a in list(setNode1) if a in list(setNode2)), 0)\n",
    "    \n",
    "    #uses a hash map to identify the first common ancestor in both lists\n",
    "    #looks for the first common ancestor in setNode1, which can also be found in setNode2 \n",
    "    #--> this might not be the closest distance between setNode1 and setNode2\n",
    "    #--> e.g., for x= [0,1,3,7,5,6] and y= [4,5,7,8,3] 7 might be closest ancestor, although algo detects 3 !\n",
    "    #distance = setNode1[firstCommonAnces] + setNode2[firstCommonAnces]\n",
    "    \n",
    "    \n",
    "    if firstCommonAnces != None:   # ISSUE: in some cases the firstCommonAnces cannot be detected!\n",
    "        ancesDistNode1 =  setNode1[firstCommonAnces] + 1 #the edge from node1 to first parent is counted as 0 by algorithm, therefore +1\n",
    "        ancesDistNode2 =  setNode2[firstCommonAnces] + 1\n",
    "        numberSkips = abs(ancesDistNode1 - ancesDistNode2)\n",
    "        numberCross = min(ancesDistNode1, ancesDistNode2)\n",
    "    else:\n",
    "        numberSkips, numberCross = (0,1)\n",
    "    return numberSkips, numberCross\n",
    "    #if all(x in crossType for x in i):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#graphList = [[1], [2, 4, 1], [3, 2, 1], [], [5, 4], [5, 4, 6], [7], []]\n",
    "#c = [[1, 4], [2], [3], [0, 5], [3, 5], []]\n",
    "#c2 = {v: k for v, k in enumerate(c)}\n",
    "#common_ancestors(c, 4, 5)\n",
    "#reverse_graph(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ebeba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List for decoding traces\n",
    "from collections import OrderedDict\n",
    "logVar[\"indexList\"] = logVar[\"activity\"].apply(lambda x: list(OrderedDict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36477c",
   "metadata": {},
   "source": [
    "### Cosine Edge Type + length (no df relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d96d6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph1:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = [['sequ', 1]]\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return np.array(self.structural_array)\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                self.structural_array[0][1] += 0\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append(['1back ',1])\n",
    "                    #self.structural_array.append(['back ',1])\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append(['2back ',2])\n",
    "                    #self.structural_array.append(['back ',2])\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append([str(x+1)+'back ',x+1])\n",
    "                    #self.structural_array.append(['back ',x+1])\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append([str(y-1)+'forward ',y-1])\n",
    "                #self.structural_array.append(['forward ' ,y-1])\n",
    "                #self.structural_array.extend((y-1)*['forward']) # -1 to exclude one edge: (A:B,C;B:C;C:[]) ...the dist A --> C is 2 without forward edge, but we are skipping only one activity\n",
    "                #self.structural_array.append(str(y-1)+'f')\n",
    "                \n",
    "            else:\n",
    "                #Possibly first check, whether two nodes connected by cross-type have identical parent\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                #self.structural_array.append(['forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]),numberSkips])\n",
    "                self.structural_array.append([str(numberCross)+'cross ',numberCross])\n",
    "                #self.structural_array.append(['cross ' ,numberCross])\n",
    "                #self.structural_array.append(str(numberSkips)+'f')\n",
    "                #self.structural_array.append(str(numberCross)+'c')\n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a606aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def transform_list_of_pairs(pairs):\n",
    "    return [pair[0] for pair in pairs]\n",
    "\n",
    "\n",
    "\n",
    "def count_entries(input_list):\n",
    "    # Count the occurrences of each unique entry in the list\n",
    "    counter = Counter(input_list)\n",
    "    \n",
    "    # Create a NumPy array from the counter dictionary\n",
    "    result = np.array([[key, count] for key, count in counter.items()], dtype=object)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "#input_list = ['sequ', '2back', '2back']\n",
    "#result = count_entries(input_list)\n",
    "#print(result)\n",
    "\n",
    "logVar[\"int_strucLengthList2\"] = logVar.apply(lambda x: Graph1(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "logVar[\"relFrequVec1\"] = logVar[\"int_strucLengthList2\"].apply(lambda x: transform_list_of_pairs(x))\n",
    "logVar[\"relFrequVec1\"] = logVar[\"relFrequVec1\"].apply(lambda x: count_entries(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "13649a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9796875\n",
      "Tri:  tensor(0.8143)\n",
      "Sil:  0.42330117342623563\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "cos_graph_dis = matrix_calc(logVar[\"relFrequVec1\"],cosineDist)\n",
    "results(cos_graph_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f449e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9921875\n",
      "Tri:  tensor(0.6659)\n",
      "Sil:  0.13056350209757522\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "agg_cos = cos1_dis + cos2_dis + cos_graph_dis\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20278ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f88c69e",
   "metadata": {},
   "source": [
    "### Jaccard Edge Type and length + df relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bd8352d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph2:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = []\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return self.structural_array\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                #self.structural_array[0][1] += 0\n",
    "                #self.structural_array.append('tree ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ')\n",
    "                self.structural_array.append('tree')\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(1))\n",
    "                    #self.structural_array.append('back ' + str(1))\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(2))\n",
    "                    #self.structural_array.append('back ' + str(2))\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(x+1))\n",
    "                    #self.structural_array.append('back ' + str(x+1))\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(y-1))\n",
    "                #self.structural_array.append('forward' + str(y-1))\n",
    "                #self.structural_array.extend((y-1)*['forward']) # -1 to exclude one edge: (A:B,C;B:C;C:[]) ...the dist A --> C is 2 without forward edge, but we are skipping only one activity\n",
    "                #self.structural_array.append(str(y-1)+'f')\n",
    "                \n",
    "            else:\n",
    "                #Possibly first check, whether two nodes connected by cross-type have identical parent\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                #self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberSkips))\n",
    "                self.structural_array.append('cross ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberCross))\n",
    "                #self.structural_array.append('cross ' + str(numberCross))\n",
    "                #self.structural_array.append(str(numberSkips)+'f')\n",
    "                #self.structural_array.append(str(numberCross)+'c')\n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7e00cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"int_strucLengthList3\"] = logVar.apply(lambda x: Graph2(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8bcbaddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.99375\n",
      "P@10: 0.9425000000000001\n",
      "Tri:  tensor(0.6234)\n",
      "Sil:  0.20391353704995793\n"
     ]
    }
   ],
   "source": [
    "#Jacc similarity based on edge types\n",
    "\n",
    "jacc_graph = matrix_calc(logVar[\"int_strucLengthList3\"],jaccard_similarity)\n",
    "results(jacc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1c9d9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9921875\n",
      "Tri:  tensor(0.6659)\n",
      "Sil:  0.13056350209757522\n"
     ]
    }
   ],
   "source": [
    "#Jacc sim based on edge types\n",
    "agg_jacc = Jacc1_dis + Jacc2_dis + jacc_graph\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e64e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9115a2c",
   "metadata": {},
   "source": [
    "## Eventually Follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7791748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial distance between strings\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def distanceSpatial(traceString, char1, char2):\n",
    "    positions_letter1 = [pos for pos, char in enumerate(traceString) if char == char1]\n",
    "    positions_letter2 = [pos for pos, char in enumerate(traceString) if char == char2]\n",
    "    \n",
    "    distList = []\n",
    "    \n",
    "\n",
    "    for i in range(len(positions_letter1)):\n",
    "        for j in range(len(positions_letter2)):\n",
    "            dist = positions_letter2[j] - positions_letter1[i]\n",
    "            if dist > 0:\n",
    "                    #print(dist)\n",
    "                distList.append(dist)\n",
    "                    \n",
    "    \n",
    "    if not distList: #distList.append(0) #in the case the char1 is after char2 asign dist 0, i.e. char2 cannot be reached from char1\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/min(distList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def commonDistance(trace1, trace2):\n",
    "    \n",
    "    commonSet = set(trace1) & set(trace2)\n",
    "\n",
    "    commonList = list(commonSet)\n",
    "    commonList.sort()\n",
    "    #print(commonList)\n",
    "\n",
    "    n = len(commonSet)\n",
    "    dist_matrix1 = np.zeros((n,n))\n",
    "    dist_matrix2 = np.zeros((n,n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix1[i,j] = distanceSpatial(trace1, commonList[i], commonList[j])\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix2[i,j] = distanceSpatial(trace2, commonList[i], commonList[j])\n",
    "    \n",
    "    #print(dist_matrix1, dist_matrix2)\n",
    "    return distance.cosine(dist_matrix1.ravel(), dist_matrix2.ravel())\n",
    "\n",
    "\n",
    "\n",
    "#x = 'ABCDEF'\n",
    "#y = 'ABCDEBCDEBCDEF'\n",
    "#z = 'ABCDEBCDEF'\n",
    "#print(dist_matrix)\n",
    "#distanceSpatial(x, 'A', 'E')\n",
    "#listVec = logVar[\"strings\"]\n",
    "#x= listVec[0]\n",
    "#y= listVec[1]\n",
    "#commonDistance(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8ab0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\la1949\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\la1949\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   1.0\n",
      "P@10: 0.9903125\n",
      "Tri:  tensor(0.4511)\n",
      "Sil:  0.07839009303944726\n"
     ]
    }
   ],
   "source": [
    "dist_matrix_evFollows = matrix_calc(logVar[\"strings\"],commonDistance)\n",
    "dist_matrix_evFollows = np.nan_to_num(dist_matrix_evFollows, nan=0)\n",
    "agg_evFollows = 0.7*dist_matrix_evFollows + 0.3*cos1_dis \n",
    "results(agg_evFollows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8ce48",
   "metadata": {},
   "source": [
    "## Maximal Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86fc5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from suffix_tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c99163ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree.maximal_repeats\n",
    "def maxRepeat(tree):\n",
    "    mrList=[]\n",
    "    for C, path in sorted(tree.maximal_repeats()):\n",
    "        mrList.append(str(path))\n",
    "    return mrList\n",
    "\n",
    "#test_tree = Tree({\"A\": \"aaacdcdcbedbccbadbdebdc\"})\n",
    "#maxRepeat(test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03645268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector based on maximal repeats\n",
    "logVar[\"mrList\"] = logVar[\"strings\"].apply(lambda x: maxRepeat(Tree({\"A\": x})))\n",
    "logVar[\"mrVector\"] = logVar[\"mrList\"].apply(lambda x: createVector(tuple(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "35fc2810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9781249999999999\n",
      "P@10: 0.9321875\n",
      "Tri:  tensor(0.4390)\n",
      "Sil:  0.13541501726007765\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on maxR\n",
    "\n",
    "cos_mr = matrix_calc(logVar[\"mrVector\"],cosineDist)\n",
    "results(cos_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ed88d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.965625\n",
      "P@10: 0.8534375\n",
      "Tri:  tensor(0.5641)\n",
      "Sil:  0.03761836925750636\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on maxR\n",
    "\n",
    "euc_mr = matrix_calc(logVar[\"mrVector\"],euclidDist)\n",
    "results(euc_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2fe484b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9781249999999999\n",
      "P@10: 0.9303125\n",
      "Tri:  tensor(0.4386)\n",
      "Sil:  0.10574590327177298\n"
     ]
    }
   ],
   "source": [
    "#Jaccard similarity based on maxR\n",
    "\n",
    "jacc_mr = matrix_calc(logVar[\"mrList\"],jaccard_similarity)\n",
    "results(jacc_mr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

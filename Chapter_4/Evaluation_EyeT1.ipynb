{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "#from scipy.stats import mannwhitneyu\n",
    "#import collections\n",
    "#from statistics import mean\n",
    "#from statistics import median\n",
    "from collections import defaultdict\n",
    "from Levenshtein import distance\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1dd85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#\n",
    "# The Dataset will only be available for the review\n",
    "#\n",
    "##############################\n",
    "\n",
    "\n",
    "log = pd.read_csv(\"eventsWithPhases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b702daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create log with Case ID based on currentQuestion + participant\n",
    "\n",
    "#first change data type of currentQuestion from int to str\n",
    "log = log.astype({'currentQuestion': str})\n",
    "log.dtypes\n",
    "\n",
    "#combine two columns\n",
    "log['case_id'] = pd.factorize(log.participant+log.currentQuestion)[0]\n",
    "\n",
    "#there are 614 cases, although 46 participants * 14 questions = 616 --> 2 cases are missing \n",
    "#print(len(log.case_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d396445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversion function\n",
    "def convert_ms_to_date(milliseconds):\n",
    "    date_obj = datetime.fromtimestamp(milliseconds / 1000.0)\n",
    "    date_string = date_obj.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    return date_string\n",
    "\n",
    "# Apply conversion function to 'milliseconds' column\n",
    "log['fixation_start'] = log['Fixation Start'].apply(convert_ms_to_date)\n",
    "log['fixation_end'] = log['Fixation End'].apply(convert_ms_to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeb7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'tabName_element' column\n",
    "log['activity'] = le.fit_transform(log['tabName_element'])\n",
    "\n",
    "#Number of unique activity values\n",
    "#log['activity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff605e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Control-flow questions\n",
    "log_select = log[['case_id', 'fixation_start', 'activity', 'Phase', 'Type1', 'Type2', 'Type3', 'Fixation Duration']]\n",
    "log_tasks = log_select.loc[log_select['Type2'] == 'Control-flow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca333c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de673391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>fixation_start</th>\n",
       "      <th>activity</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Type1</th>\n",
       "      <th>Type2</th>\n",
       "      <th>Type3</th>\n",
       "      <th>Fixation Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31794</th>\n",
       "      <td>106</td>\n",
       "      <td>1970-01-01 01:38:40.855554</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>83.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31795</th>\n",
       "      <td>106</td>\n",
       "      <td>1970-01-01 01:38:47.950454</td>\n",
       "      <td>412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>66.624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_id              fixation_start  activity Phase  Type1  \\\n",
       "31794      106  1970-01-01 01:38:40.855554        50   NaN  Local   \n",
       "31795      106  1970-01-01 01:38:47.950454       412   NaN  Local   \n",
       "\n",
       "              Type2     Type3  Fixation Duration  \n",
       "31794  Control-flow  Ordering             83.316  \n",
       "31795  Control-flow  Ordering             66.624  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The experiment session with case_id 106 only contains 2 fixations and is therefore removed from the evaluation\n",
    "#The error porbably occured since the participant involuntarily clicked on the 'next' button to go to the next task\n",
    "log_tasks[log_tasks['case_id'] == 106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1f823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>fixation_start</th>\n",
       "      <th>activity</th>\n",
       "      <th>Phase</th>\n",
       "      <th>Type1</th>\n",
       "      <th>Type2</th>\n",
       "      <th>Type3</th>\n",
       "      <th>Fixation Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:15.800704</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>83.4080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:16.708983</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>124.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:16.883977</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>66.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:17.900487</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>83.3050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1970-01-01 02:35:18.375424</td>\n",
       "      <td>82</td>\n",
       "      <td>search</td>\n",
       "      <td>Local</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Exclusiveness</td>\n",
       "      <td>108.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173748</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:17.900596</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>191.6515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173749</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.142225</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>283.3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173750</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.442194</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>141.6530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173751</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.642171</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>191.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173752</th>\n",
       "      <td>610</td>\n",
       "      <td>1970-01-01 03:15:18.933816</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Global</td>\n",
       "      <td>Control-flow</td>\n",
       "      <td>Ordering</td>\n",
       "      <td>508.2945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98671 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        case_id              fixation_start  activity   Phase   Type1  \\\n",
       "0             0  1970-01-01 02:35:15.800704        82  search   Local   \n",
       "1             0  1970-01-01 02:35:16.708983        82  search   Local   \n",
       "2             0  1970-01-01 02:35:16.883977        82  search   Local   \n",
       "3             0  1970-01-01 02:35:17.900487        82  search   Local   \n",
       "4             0  1970-01-01 02:35:18.375424        82  search   Local   \n",
       "...         ...                         ...       ...     ...     ...   \n",
       "173748      610  1970-01-01 03:15:17.900596        88     NaN  Global   \n",
       "173749      610  1970-01-01 03:15:18.142225       179     NaN  Global   \n",
       "173750      610  1970-01-01 03:15:18.442194       179     NaN  Global   \n",
       "173751      610  1970-01-01 03:15:18.642171        50     NaN  Global   \n",
       "173752      610  1970-01-01 03:15:18.933816       179     NaN  Global   \n",
       "\n",
       "               Type2          Type3  Fixation Duration  \n",
       "0       Control-flow  Exclusiveness            83.4080  \n",
       "1       Control-flow  Exclusiveness           124.9670  \n",
       "2       Control-flow  Exclusiveness            66.6000  \n",
       "3       Control-flow  Exclusiveness            83.3050  \n",
       "4       Control-flow  Exclusiveness           108.3100  \n",
       "...              ...            ...                ...  \n",
       "173748  Control-flow       Ordering           191.6515  \n",
       "173749  Control-flow       Ordering           283.3150  \n",
       "173750  Control-flow       Ordering           141.6530  \n",
       "173751  Control-flow       Ordering           191.6490  \n",
       "173752  Control-flow       Ordering           508.2945  \n",
       "\n",
       "[98671 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_tasks = log_tasks.drop(log_tasks[log_tasks['case_id'] == 106].index)\n",
    "log_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "118b1ec6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_tasks['taskType'] = log_tasks['Type1'] + log_tasks['Type3']\n",
    "log_group = log_tasks.groupby(['case_id'])['taskType'].apply(list).reset_index()\n",
    "log_group['task'] = log_group['taskType'].apply(lambda x: str(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9b0f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[82, 82, 82, 82, 82, 177, 82, 82, 82, 82, 82, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>[82, 82, 82, 181, 82, 82, 82, 82, 82, 82, 181,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>[90, 82, 183, 82, 82, 82, 82, 82, 183, 77, 75,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>[86, 82, 82, 82, 82, 178, 178, 82, 82, 82, 82,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>606</td>\n",
       "      <td>[82, 59, 120, 122, 183, 120, 120, 183, 59, 183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>607</td>\n",
       "      <td>[82, 82, 82, 102, 175, 175, 175, 175, 175, 175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>608</td>\n",
       "      <td>[86, 86, 82, 108, 109, 86, 178, 86, 178, 108, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>609</td>\n",
       "      <td>[82, 195, 195, 195, 195, 195, 195, 195, 195, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>610</td>\n",
       "      <td>[82, 82, 112, 112, 179, 82, 179, 82, 179, 82, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id                                           activity\n",
       "0          0  [82, 82, 82, 82, 82, 177, 82, 82, 82, 82, 82, ...\n",
       "1          4  [82, 82, 82, 181, 82, 82, 82, 82, 82, 82, 181,...\n",
       "2          5  [90, 82, 183, 82, 82, 82, 82, 82, 183, 77, 75,...\n",
       "3          6  [82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 8...\n",
       "4          7  [86, 82, 82, 82, 82, 178, 178, 82, 82, 82, 82,...\n",
       "..       ...                                                ...\n",
       "344      606  [82, 59, 120, 122, 183, 120, 120, 183, 59, 183...\n",
       "345      607  [82, 82, 82, 102, 175, 175, 175, 175, 175, 175...\n",
       "346      608  [86, 86, 82, 108, 109, 86, 178, 86, 178, 108, ...\n",
       "347      609  [82, 195, 195, 195, 195, 195, 195, 195, 195, 1...\n",
       "348      610  [82, 82, 112, 112, 179, 82, 179, 82, 179, 82, ...\n",
       "\n",
       "[349 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create trace log\n",
    "logVar = log_tasks.groupby(['case_id'])['activity'].apply(list).reset_index()\n",
    "#len(logVar[\"activity\"][310])\n",
    "logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861f2374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar[\"length\"] = logVar[\"activity\"].apply(lambda x: len(x))\n",
    "#logVar.sort_values(by=['length'])\n",
    "logVar['length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b347e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logVar['length'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4777137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5,\n",
       " 1: 0,\n",
       " 2: 3,\n",
       " 3: 4,\n",
       " 4: 7,\n",
       " 5: 6,\n",
       " 6: 2,\n",
       " 7: 1,\n",
       " 8: 5,\n",
       " 9: 0,\n",
       " 10: 3,\n",
       " 11: 4,\n",
       " 12: 7,\n",
       " 13: 6,\n",
       " 14: 2,\n",
       " 15: 1,\n",
       " 16: 5,\n",
       " 17: 0,\n",
       " 18: 3,\n",
       " 19: 4,\n",
       " 20: 7,\n",
       " 21: 6,\n",
       " 22: 2,\n",
       " 23: 1,\n",
       " 24: 5,\n",
       " 25: 0,\n",
       " 26: 3,\n",
       " 27: 4,\n",
       " 28: 7,\n",
       " 29: 6,\n",
       " 30: 2,\n",
       " 31: 1,\n",
       " 32: 5,\n",
       " 33: 0,\n",
       " 34: 3,\n",
       " 35: 4,\n",
       " 36: 7,\n",
       " 37: 6,\n",
       " 38: 2,\n",
       " 39: 1,\n",
       " 40: 5,\n",
       " 41: 0,\n",
       " 42: 3,\n",
       " 43: 4,\n",
       " 44: 7,\n",
       " 45: 6,\n",
       " 46: 2,\n",
       " 47: 1,\n",
       " 48: 5,\n",
       " 49: 0,\n",
       " 50: 3,\n",
       " 51: 4,\n",
       " 52: 7,\n",
       " 53: 6,\n",
       " 54: 2,\n",
       " 55: 1,\n",
       " 56: 5,\n",
       " 57: 0,\n",
       " 58: 3,\n",
       " 59: 4,\n",
       " 60: 7,\n",
       " 61: 2,\n",
       " 62: 1,\n",
       " 63: 5,\n",
       " 64: 0,\n",
       " 65: 3,\n",
       " 66: 4,\n",
       " 67: 7,\n",
       " 68: 6,\n",
       " 69: 2,\n",
       " 70: 1,\n",
       " 71: 5,\n",
       " 72: 0,\n",
       " 73: 3,\n",
       " 74: 4,\n",
       " 75: 7,\n",
       " 76: 6,\n",
       " 77: 2,\n",
       " 78: 1,\n",
       " 79: 5,\n",
       " 80: 0,\n",
       " 81: 3,\n",
       " 82: 4,\n",
       " 83: 7,\n",
       " 84: 6,\n",
       " 85: 2,\n",
       " 86: 1,\n",
       " 87: 5,\n",
       " 88: 0,\n",
       " 89: 3,\n",
       " 90: 4,\n",
       " 91: 7,\n",
       " 92: 6,\n",
       " 93: 2,\n",
       " 94: 1,\n",
       " 95: 5,\n",
       " 96: 0,\n",
       " 97: 3,\n",
       " 98: 4,\n",
       " 99: 7,\n",
       " 100: 6,\n",
       " 101: 2,\n",
       " 102: 1,\n",
       " 103: 5,\n",
       " 104: 0,\n",
       " 105: 3,\n",
       " 106: 4,\n",
       " 107: 7,\n",
       " 108: 6,\n",
       " 109: 2,\n",
       " 110: 1,\n",
       " 111: 5,\n",
       " 112: 0,\n",
       " 113: 3,\n",
       " 114: 4,\n",
       " 115: 7,\n",
       " 116: 6,\n",
       " 117: 2,\n",
       " 118: 1,\n",
       " 119: 5,\n",
       " 120: 0,\n",
       " 121: 3,\n",
       " 122: 4,\n",
       " 123: 7,\n",
       " 124: 6,\n",
       " 125: 2,\n",
       " 126: 1,\n",
       " 127: 5,\n",
       " 128: 0,\n",
       " 129: 3,\n",
       " 130: 4,\n",
       " 131: 7,\n",
       " 132: 6,\n",
       " 133: 2,\n",
       " 134: 1,\n",
       " 135: 5,\n",
       " 136: 0,\n",
       " 137: 3,\n",
       " 138: 4,\n",
       " 139: 7,\n",
       " 140: 6,\n",
       " 141: 2,\n",
       " 142: 1,\n",
       " 143: 5,\n",
       " 144: 0,\n",
       " 145: 3,\n",
       " 146: 4,\n",
       " 147: 7,\n",
       " 148: 6,\n",
       " 149: 2,\n",
       " 150: 1,\n",
       " 151: 5,\n",
       " 152: 0,\n",
       " 153: 3,\n",
       " 154: 4,\n",
       " 155: 7,\n",
       " 156: 6,\n",
       " 157: 2,\n",
       " 158: 1,\n",
       " 159: 5,\n",
       " 160: 0,\n",
       " 161: 3,\n",
       " 162: 4,\n",
       " 163: 7,\n",
       " 164: 6,\n",
       " 165: 2,\n",
       " 166: 1,\n",
       " 167: 5,\n",
       " 168: 0,\n",
       " 169: 3,\n",
       " 170: 4,\n",
       " 171: 7,\n",
       " 172: 6,\n",
       " 173: 2,\n",
       " 174: 1,\n",
       " 175: 5,\n",
       " 176: 0,\n",
       " 177: 3,\n",
       " 178: 4,\n",
       " 179: 7,\n",
       " 180: 6,\n",
       " 181: 2,\n",
       " 182: 1,\n",
       " 183: 5,\n",
       " 184: 0,\n",
       " 185: 3,\n",
       " 186: 4,\n",
       " 187: 7,\n",
       " 188: 6,\n",
       " 189: 2,\n",
       " 190: 1,\n",
       " 191: 5,\n",
       " 192: 0,\n",
       " 193: 3,\n",
       " 194: 4,\n",
       " 195: 7,\n",
       " 196: 6,\n",
       " 197: 2,\n",
       " 198: 1,\n",
       " 199: 5,\n",
       " 200: 0,\n",
       " 201: 3,\n",
       " 202: 4,\n",
       " 203: 7,\n",
       " 204: 6,\n",
       " 205: 2,\n",
       " 206: 1,\n",
       " 207: 5,\n",
       " 208: 0,\n",
       " 209: 3,\n",
       " 210: 4,\n",
       " 211: 7,\n",
       " 212: 6,\n",
       " 213: 2,\n",
       " 214: 1,\n",
       " 215: 5,\n",
       " 216: 0,\n",
       " 217: 3,\n",
       " 218: 4,\n",
       " 219: 7,\n",
       " 220: 6,\n",
       " 221: 2,\n",
       " 222: 1,\n",
       " 223: 5,\n",
       " 224: 0,\n",
       " 225: 3,\n",
       " 226: 4,\n",
       " 227: 7,\n",
       " 228: 6,\n",
       " 229: 2,\n",
       " 230: 1,\n",
       " 231: 5,\n",
       " 232: 0,\n",
       " 233: 3,\n",
       " 234: 4,\n",
       " 235: 7,\n",
       " 236: 6,\n",
       " 237: 2,\n",
       " 238: 1,\n",
       " 239: 5,\n",
       " 240: 0,\n",
       " 241: 3,\n",
       " 242: 4,\n",
       " 243: 7,\n",
       " 244: 6,\n",
       " 245: 2,\n",
       " 246: 1,\n",
       " 247: 5,\n",
       " 248: 0,\n",
       " 249: 3,\n",
       " 250: 4,\n",
       " 251: 7,\n",
       " 252: 6,\n",
       " 253: 2,\n",
       " 254: 1,\n",
       " 255: 5,\n",
       " 256: 0,\n",
       " 257: 3,\n",
       " 258: 4,\n",
       " 259: 7,\n",
       " 260: 6,\n",
       " 261: 2,\n",
       " 262: 1,\n",
       " 263: 5,\n",
       " 264: 0,\n",
       " 265: 3,\n",
       " 266: 4,\n",
       " 267: 7,\n",
       " 268: 6,\n",
       " 269: 2,\n",
       " 270: 1,\n",
       " 271: 5,\n",
       " 272: 0,\n",
       " 273: 3,\n",
       " 274: 4,\n",
       " 275: 7,\n",
       " 276: 6,\n",
       " 277: 2,\n",
       " 278: 1,\n",
       " 279: 5,\n",
       " 280: 0,\n",
       " 281: 3,\n",
       " 282: 4,\n",
       " 283: 7,\n",
       " 284: 6,\n",
       " 285: 2,\n",
       " 286: 1,\n",
       " 287: 5,\n",
       " 288: 0,\n",
       " 289: 3,\n",
       " 290: 4,\n",
       " 291: 7,\n",
       " 292: 6,\n",
       " 293: 2,\n",
       " 294: 1,\n",
       " 295: 5,\n",
       " 296: 0,\n",
       " 297: 3,\n",
       " 298: 4,\n",
       " 299: 7,\n",
       " 300: 6,\n",
       " 301: 2,\n",
       " 302: 1,\n",
       " 303: 5,\n",
       " 304: 0,\n",
       " 305: 3,\n",
       " 306: 4,\n",
       " 307: 7,\n",
       " 308: 6,\n",
       " 309: 2,\n",
       " 310: 1,\n",
       " 311: 5,\n",
       " 312: 0,\n",
       " 313: 3,\n",
       " 314: 4,\n",
       " 315: 7,\n",
       " 316: 6,\n",
       " 317: 2,\n",
       " 318: 1,\n",
       " 319: 5,\n",
       " 320: 0,\n",
       " 321: 3,\n",
       " 322: 4,\n",
       " 323: 7,\n",
       " 324: 6,\n",
       " 325: 2,\n",
       " 326: 1,\n",
       " 327: 5,\n",
       " 328: 0,\n",
       " 329: 3,\n",
       " 330: 4,\n",
       " 331: 7,\n",
       " 332: 6,\n",
       " 333: 2,\n",
       " 334: 1,\n",
       " 335: 5,\n",
       " 336: 0,\n",
       " 337: 3,\n",
       " 338: 4,\n",
       " 339: 7,\n",
       " 340: 6,\n",
       " 341: 2,\n",
       " 342: 1,\n",
       " 343: 0,\n",
       " 344: 3,\n",
       " 345: 4,\n",
       " 346: 7,\n",
       " 347: 6,\n",
       " 348: 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dictionary with true labels\n",
    "log_group['label'] = le.fit_transform(log_group['task'])\n",
    "labelDict1 = log_group['label'].to_dict()\n",
    "labelDict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71421cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    44\n",
       "1    43\n",
       "2    44\n",
       "3    44\n",
       "4    44\n",
       "5    43\n",
       "6    43\n",
       "7    44\n",
       "Name: case_id, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_dis = log_group.groupby(['label'])['case_id'].apply(list).reset_index()\n",
    "trace_dis['case_id'].str.len()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0863e5",
   "metadata": {},
   "source": [
    "## Evaluation based on Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8407d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 8,  9,  5, 4, 0]])\\n\\nlabelDict2 = {0:1,1:2,2:1,3:3,4:3}\\nNearestNeighbor(x, labelDict2)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NearestNeighbor(matrix, labelDict):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        #delete 0 in array matrix[i] for distance between (i,i)\n",
    "        x = np.delete(matrix[i], i) \n",
    "        #identify min distance/value in array\n",
    "        y = min(x)\n",
    "        \n",
    "        #identify position of y AND select first position/pair appearing in the array in case there are muliple pairs with identical min distance\n",
    "        nearestNeighbor = np.where(x == y)[0] #problem if multiple positions??\n",
    "        nearestNeighbor = int(nearestNeighbor[0])\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "        \n",
    "        #for label comparison add +1 to dict position if position occurs after i (because of the deletion of 0 at the beginning)\n",
    "        if nearestNeighbor >= i:\n",
    "            if trueLabel == labelDict[nearestNeighbor + 1]:\n",
    "                clusterMeasuredCount[trueLabel] += 1         \n",
    "        else:\n",
    "            if trueLabel == labelDict[nearestNeighbor]:\n",
    "                clusterMeasuredCount[trueLabel] += 1\n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterMeasuredCount)\n",
    "    #print(clusterTrueCount)\n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 8,  9,  5, 4, 0]])\n",
    "\n",
    "labelDict2 = {0:1,1:2,2:1,3:3,4:3}\n",
    "NearestNeighbor(x, labelDict2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aae66",
   "metadata": {},
   "source": [
    "## Evaluation based on Precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24a0d082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx3 = np.array([[ 0, 12,  8, 10, 12],\\n       [5,  0,  9, 14, 17],\\n       [ 8,  9,  0, 12, 10],\\n       [ 8,  9,  1, 0, 10],\\n       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\\n\\nlabelDict3 = {0:1,1:2,2:1,3:3,4:3}\\nPrecisionAtK(x3, labelDict3, 2)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_key_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    #return None  # Return None if the value is not found in the dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PrecisionAtK(matrix, labelDict, k):\n",
    "    \n",
    "    clusterLabel = set(labelDict.values())\n",
    "    clusterMeasuredCount = dict.fromkeys(list(clusterLabel), 0)\n",
    "    \n",
    "    \n",
    "    for i in range(len(matrix)):\n",
    "        \n",
    "        #true Label for i\n",
    "        trueLabel = labelDict[i]\n",
    "\n",
    "        \n",
    "        #delete 0 in array matrix[i] for distance between (i,i)   \n",
    "        x = np.delete(matrix[i], i)\n",
    "        \n",
    "        #get all k minimum values\n",
    "        nnValues = []\n",
    "        for l in range(k):\n",
    "            y = min(x)\n",
    "            nnValues.append(y)\n",
    "            x = np.delete(x, np.where(x == y)[0][0])\n",
    "        \n",
    "        \n",
    "        #get location of minimum values\n",
    "        z = np.delete(matrix[i], i)\n",
    "        \n",
    "        #create dict from array with {position:value}\n",
    "        my_dict = {}\n",
    "        for m in range(len(z)):\n",
    "            my_dict[m] = z[m]\n",
    "        \n",
    "        key_list = []\n",
    "        for n in nnValues:\n",
    "            key = get_key_by_value(my_dict, n)\n",
    "            key_list.append(key)\n",
    "            del my_dict[key]\n",
    "        \n",
    "        for o in range(k):\n",
    "            position = list(key_list)[o]\n",
    "\n",
    "            if position >= i:\n",
    "                if trueLabel == labelDict[position + 1]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            else:\n",
    "                if trueLabel == labelDict[position]:\n",
    "                    clusterMeasuredCount[trueLabel] += 1\n",
    "\n",
    "            #transform array to dict\n",
    "            #get key value from dict\n",
    "            #compare label\n",
    "            #remove key+value from dict\n",
    "\n",
    "            #Need exception in case y == 0 ??\n",
    "            \n",
    "    \n",
    "    #Count the (true) number of traces per label/attribute\n",
    "    clusterTrueCount = {}\n",
    "    for i in clusterLabel:\n",
    "        clusterTrueCount[i] = list(labelDict.values()).count(i)\n",
    "    \n",
    "    #Divide number of nearest neighbours with identical label by the respective (true number) of traces with this label\n",
    "    metric = 0\n",
    "    for i in clusterLabel:\n",
    "        metric += clusterMeasuredCount[i] / k / clusterTrueCount[i]\n",
    "        \n",
    "    #print(clusterMeasuredCount, clusterTrueCount, metric)\n",
    "    \n",
    "    #print(clusterLabel)\n",
    "    #print(metric)\n",
    "    return metric / len(clusterLabel)\n",
    "\n",
    "\n",
    "'''\n",
    "x3 = np.array([[ 0, 12,  8, 10, 12],\n",
    "       [5,  0,  9, 14, 17],\n",
    "       [ 8,  9,  0, 12, 10],\n",
    "       [ 8,  9,  1, 0, 10],\n",
    "       [ 4,  8,  4, 4, 0]]) # --> Issue: How to select NN when identical distance values !!!\n",
    "\n",
    "labelDict3 = {0:1,1:2,2:1,3:3,4:3}\n",
    "PrecisionAtK(x3, labelDict3, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18183bf",
   "metadata": {},
   "source": [
    "## Triplet\n",
    "\n",
    "see: https://towardsdatascience.com/triplet-loss-advanced-intro-49a07b7d8905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8138ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "836bd93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_mask(labels):\n",
    "    \n",
    "    # step 1 - get a mask for distinct indices\n",
    "    ###print('labels', labels)\n",
    "    # shape: (batch_size, batch_size)\n",
    "    indices_equal = torch.eye(labels.size()[0], dtype=torch.bool, device=labels.device)\n",
    "    ###print('equal', indices_equal)\n",
    "    indices_not_equal = torch.logical_not(indices_equal)\n",
    "    ###print('not_equal', indices_not_equal)\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_not_equal_j = indices_not_equal.unsqueeze(2)\n",
    "    ###print('i_not_j - unsqueeze2', i_not_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_not_equal_k = indices_not_equal.unsqueeze(1)\n",
    "    ###print('i_not_k - unsqueeze1', i_not_equal_k)\n",
    "    # shape: (1, batch_size, batch_size)\n",
    "    j_not_equal_k = indices_not_equal.unsqueeze(0)\n",
    "    ###print('j_not_k - unsqueeze0', i_not_equal_k)\n",
    "    # Shape: (batch_size, batch_size, batch_size)\n",
    "    distinct_indices = torch.logical_and(torch.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "    ###print('distinct!!!!', distinct_indices)\n",
    "\n",
    "    # step 2 - get a mask for valid anchor-positive-negative triplets\n",
    "    # shape: (batch_size, batch_size)\n",
    "    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n",
    "    ###print('labels_equal', labels_equal)\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    i_equal_j = labels_equal.unsqueeze(2)\n",
    "    ###print('i_equal_j', i_equal_j)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    i_equal_k = labels_equal.unsqueeze(1)\n",
    "    ###print('i_equal_k', i_equal_k)\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    valid_indices = torch.logical_and(i_equal_j, torch.logical_not(i_equal_k))\n",
    "    ###print('valid_indices!!!', valid_indices)\n",
    "    \n",
    "\n",
    "    # step 3 - combine two masks\n",
    "    mask = torch.logical_and(distinct_indices, valid_indices)\n",
    "    ###print('mask!!', mask)\n",
    "    return mask\n",
    "\n",
    "    \"\"\"compute a mask for valid triplets\n",
    "    Args:\n",
    "        labels: Batch of integer labels. shape: (batch_size,)\n",
    "    Returns:\n",
    "        Mask tensor to indicate which triplets are actually valid. Shape: (batch_size, batch_size, batch_size)\n",
    "        A triplet is valid if:\n",
    "        `labels[i] == labels[j] and labels[i] != labels[k]`\n",
    "        and `i`, `j`, `k` are different.\n",
    "    \"\"\"\n",
    "    \n",
    "class custom_activation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(custom_activation, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x[x>0] = 1\n",
    "        x[x<=0] = 0\n",
    "        return x\n",
    "\n",
    "\n",
    "class BatchAllTtripletLoss(nn.Module):\n",
    "  \"\"\"Uses all valid triplets to compute Triplet loss\n",
    "\n",
    "  Args:\n",
    "    margin: Margin value in the Triplet Loss equation\n",
    "  \"\"\"\n",
    "  def __init__(self, margin=0): #default margin = 0\n",
    "    super().__init__()\n",
    "    self.margin = margin\n",
    "    self.relu = nn.ReLU() #new\n",
    "    self.custom = custom_activation()\n",
    "    \n",
    "  def forward(self, distance_matrix, labels):\n",
    "    \"\"\"computes loss value.\n",
    "\n",
    "    Args:\n",
    "      embeddings: Batch of embeddings, e.g., output of the encoder. shape: (batch_size, embedding_dim)\n",
    "      labels: Batch of integer labels associated with embeddings. shape: (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "      Scalar loss value.\n",
    "    \"\"\"\n",
    "    # step 1 - convert to tensor format\n",
    "    distance_matrix = torch.tensor(distance_matrix)\n",
    "    labels = torch.tensor(list(labels.values()))\n",
    "\n",
    "\n",
    "    # step 2 - compute loss values for all triplets by applying broadcasting to distance matrix\n",
    "\n",
    "    # shape: (batch_size, batch_size, 1)\n",
    "    anchor_positive_dists = distance_matrix.unsqueeze(2)\n",
    "    # shape: (batch_size, 1, batch_size)\n",
    "    anchor_negative_dists = distance_matrix.unsqueeze(1)\n",
    "    # get loss values for all possible n^3 triplets\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    triplet_loss = anchor_negative_dists - anchor_positive_dists + self.margin\n",
    "    ###print('tl0',triplet_loss)\n",
    "\n",
    "    # step 3 - filter out invalid or easy triplets by setting their loss values to 0\n",
    "\n",
    "    # shape: (batch_size, batch_size, batch_size)\n",
    "    mask = get_triplet_mask(labels)\n",
    "    ###print('mask', mask)\n",
    "    triplet_loss *= mask\n",
    "    ###print(triplet_loss)\n",
    "    ###print('tl1:', triplet_loss)\n",
    "    # easy triplets have negative loss values\n",
    "    \n",
    "    triplet_loss = self.custom(triplet_loss)\n",
    "    ###print(triplet_loss)\n",
    "    #triplet_loss = F.relu(triplet_loss)\n",
    "\n",
    "    # step 4 - compute scalar loss value by averaging positive losses\n",
    "    \n",
    "    triLossNonZero = (triplet_loss != 0).nonzero(as_tuple=True)\n",
    "    labelTorchUnique = torch.unique(labels, return_counts=True)\n",
    "    \n",
    "    nonZero = len(triLossNonZero[0])\n",
    "    triLossSum = []\n",
    "    for i in range(nonZero):\n",
    "        #Identify L_a --> In Class\n",
    "        t1 = triLossNonZero[0][i]\n",
    "        labelIn = labels[t1]\n",
    "        positionIn = int((labelTorchUnique[0] == labelIn).nonzero(as_tuple=False))\n",
    "        countIn = labelTorchUnique[1][positionIn]\n",
    "\n",
    "        #Identify L_b --> Out Class\n",
    "        t3 = triLossNonZero[2][i]\n",
    "        labelOut = labels[t3]\n",
    "        positionOut = int((labelTorchUnique[0] == labelOut).nonzero(as_tuple=False))\n",
    "        countOut = labelTorchUnique[1][positionOut]\n",
    "\n",
    "        #Calculate loss\n",
    "        value = (1/countIn)*(1/countIn)*(1/countOut)  \n",
    "        ###print(countIn)\n",
    "        ###print(countOut)\n",
    "        triLossSum.append(value)\n",
    "    \n",
    "    #finally divide by |A|^2-|A|\n",
    "    A = len(labelTorchUnique[0])  \n",
    "    lossValue = sum(triLossSum) / (A*A-A)\n",
    "        \n",
    "    #OLD\n",
    "    #E_triplet = (1 / (A^2 - A)) *\n",
    "    #num_positive_losses = (triplet_loss > eps).float().sum()\n",
    "    #print(num_positive_losses)\n",
    "    #print(triplet_loss.sum())\n",
    "    #triplet_loss = triplet_loss.sum() / (num_positive_losses + eps)\n",
    "    \n",
    "\n",
    "    return lossValue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12a92e",
   "metadata": {},
   "source": [
    "## Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26715870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def Silhouette(distMatrix, labelDict):\n",
    "    labelDictList = list(labelDict.values())\n",
    "    return metrics.silhouette_score(distMatrix, labelDictList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a41b8",
   "metadata": {},
   "source": [
    "## Ground truth comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28bc49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary with true labels\n",
    "log_group['label'] = le.fit_transform(log_group['task'])\n",
    "labelDict1 = log_group['label'].to_dict()\n",
    "#labelDict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b448163",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"c:n_chr\"] = logVar[\"activity\"].apply(lambda x: [chr(i) for i in x])\n",
    "logVar[\"strings\"] = logVar[\"c:n_chr\"].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c563b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_calc(features, distance):\n",
    "    n = len(features)\n",
    "    dist_matrix = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix[i,j] = distance(features[i], features[j])\n",
    "            dist_matrix[j,i] = dist_matrix[i,j]\n",
    "    \n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf525c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(DistMatrix):\n",
    "    print('NN:   ' + str(NearestNeighbor(DistMatrix, labelDict1)))\n",
    "    print('P@10: ' + str(PrecisionAtK(DistMatrix, labelDict1, 10)))\n",
    "\n",
    "    triplet = BatchAllTtripletLoss()\n",
    "    print('Tri:  ' + str(triplet.forward(DistMatrix,labelDict1)))\n",
    "    print('Sil:  ' + str(Silhouette(DistMatrix, labelDict1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f908a1",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "babbef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 267, 316, ..., 263, 274, 332],\n",
       "       [267,   0, 311, ..., 188, 180, 311],\n",
       "       [316, 311,   0, ..., 333, 307, 319],\n",
       "       ...,\n",
       "       [263, 188, 333, ...,   0, 194, 345],\n",
       "       [274, 180, 307, ..., 194,   0, 351],\n",
       "       [332, 311, 319, ..., 345, 351,   0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "List = list(logVar[\"strings\"])\n",
    "\n",
    "dist_matrix = np.zeros((len(List),len(List)),dtype=int)\n",
    "\n",
    "for i in range(0,len(List)):\n",
    "    for j in range(0,len(List)):\n",
    "        dist_matrix[i,j] = distance(List[i],List[j])\n",
    "\n",
    "lev_dis = dist_matrix\n",
    "lev_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "167bdc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.8972647991543341\n",
      "P@10: 0.7623678646934461\n",
      "Tri:  tensor(0.6691)\n",
      "Sil:  -0.22453133331372324\n"
     ]
    }
   ],
   "source": [
    "results(lev_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1e509",
   "metadata": {},
   "source": [
    "### Normalized Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15cb3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "List = logVar[\"strings\"]\n",
    "\n",
    "n = len(List)\n",
    "dist_matrix = np.zeros((n,n))    # initialize distance matrix to a square of zeros\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i, n):\n",
    "        dist_matrix[i,j] = distance(List[i], List[j]) / max(len(List[i]),len(List[j]))\n",
    "        dist_matrix[j,i] = dist_matrix[i,j]       # for the symmetric part, no computation\n",
    "\n",
    "lev_dis_norm = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e1e0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9371696617336153\n",
      "P@10: 0.8652946617336154\n",
      "Tri:  tensor(0.8727)\n",
      "Sil:  0.10736466133082465\n"
     ]
    }
   ],
   "source": [
    "results(lev_dis_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c1c26",
   "metadata": {},
   "source": [
    "### Cosine based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af2a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 1-gram\n",
    "\n",
    "def createVector(charList):\n",
    "    #dtype = [('structure', 'S10'), ('relfrequ', float)]\n",
    "    arrayList = np.array(charList)\n",
    "    unique, counts = np.unique(arrayList, return_counts=True)\n",
    "    #calculate relative frequency\n",
    "    relFrequList = np.array((unique, counts)).T\n",
    "    uniqueList = list(unique)\n",
    "    return relFrequList[relFrequList[:, 0].argsort()]\n",
    "    #check completeness\n",
    "    #if 'tree' not in uniqueList:\n",
    "        #relFrequList = np.append(relFrequList, np.array([['tree', 0]]), axis=0)\n",
    "        #print(relFrequList)\n",
    "\n",
    "        \n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"1-gram\"] = logVar[\"c:n_chr\"].apply(lambda x: createVector(tuple(x)))\n",
    "#logVar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13f472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignArrays(array1, array2):\n",
    "    commonSet = set(array1[:,0]).union(array2[:,0])\n",
    "    \n",
    "    for i in commonSet:\n",
    "        if i not in array1[:,0]:\n",
    "            array1 = np.append(array1, np.array([[i, '0']]), axis=0)\n",
    "        if i not in array2[:,0]:\n",
    "            array2 = np.append(array2, np.array([[i, '0']]), axis=0)\n",
    "    return array1[array1[:, 0].argsort()], array2[array2[:, 0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84280423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def cosineDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(int)\n",
    "    b = Vector2[:,1].astype(int)\n",
    "    dist_matrix = distance.cosine(a, b)\n",
    "    return dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b437c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine distance based on 1-gram\n",
    "\n",
    "cos1_dis = matrix_calc(logVar[\"1-gram\"],cosineDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d60975e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9772727272727273\n",
      "P@10: 0.9241477272727272\n",
      "Tri:  tensor(0.8901)\n",
      "Sil:  0.22334463230599214\n"
     ]
    }
   ],
   "source": [
    "results(cos1_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535cee1",
   "metadata": {},
   "source": [
    "### Cosine based on 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef182177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change data format from string to list of unique characters\n",
    "#logVar[\"charList\"] = logVar[\"trace_variant\"].apply(lambda x: list(x))\n",
    "#logVar\n",
    "\n",
    "def df_list(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList)):\n",
    "        new = ''.join(extList[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"dfList\"] = logVar[\"c:n_chr\"].apply(lambda x: df_list(x))\n",
    "#logVar\n",
    "\n",
    "logVar[\"2-gram\"] = logVar[\"dfList\"].apply(lambda x: createVector(x))\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "888a3701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9515063424947147\n",
      "P@10: 0.8421643763213531\n",
      "Tri:  tensor(0.8221)\n",
      "Sil:  0.08707110224755636\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 2-gram\n",
    "\n",
    "cos2_dis = matrix_calc(logVar[\"2-gram\"],cosineDist)\n",
    "results(cos2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "956c7dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create 3-gram\n",
    "\n",
    "#Change data format from string to list of unique characters\n",
    "logVar[\"charList\"] = logVar[\"c:n_chr\"].apply(lambda x: list(x))\n",
    "\n",
    "def df_list2(list_of_char):\n",
    "    extList = list_of_char.copy()\n",
    "    extList.insert(0, '*') \n",
    "    extList.append('$')\n",
    "    list_new = []\n",
    "    for i in range(len(extList) - 1):\n",
    "        new = ''.join(extList[i:i+3])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    return list_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74b9479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"dfList2\"] = logVar[\"charList\"].apply(lambda x: df_list2(x))\n",
    "logVar[\"3-gram\"] = logVar[\"dfList2\"].apply(lambda x: createVector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b0086b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.8682610993657506\n",
      "P@10: 0.7532835623678646\n",
      "Tri:  tensor(0.7620)\n",
      "Sil:  -0.0012724927871462736\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on 3-gram\n",
    "\n",
    "cos3_dis = matrix_calc(logVar[\"3-gram\"],cosineDist)\n",
    "results(cos3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb20c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9715248414376322\n",
      "P@10: 0.8948929704016915\n",
      "Tri:  tensor(0.8620)\n",
      "Sil:  0.1573427550043208\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate1 = cos1_dis + cos2_dis\n",
    "results(aggregate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e3c3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9543472515856237\n",
      "P@10: 0.8604981501057083\n",
      "Tri:  tensor(0.8380)\n",
      "Sil:  0.10582398018892779\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "aggregate2 = cos1_dis + cos2_dis + cos3_dis\n",
    "results(aggregate2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b76886",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a28527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "# see https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy\n",
    "\n",
    "def euclidDist(frequVector1, frequVector2):\n",
    "    Vector1, Vector2 = alignArrays(frequVector1, frequVector2)\n",
    "    a = Vector1[:,1].astype(float)\n",
    "    b = Vector2[:,1].astype(float)\n",
    "    euclidean_dist = np.linalg.norm(a-b)\n",
    "    return euclidean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2331cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9542811839323467\n",
      "P@10: 0.8622225158562368\n",
      "Tri:  tensor(0.7647)\n",
      "Sil:  -0.05617607680390533\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 1-gram\n",
    "\n",
    "euc1_dis = matrix_calc(logVar[\"1-gram\"],euclidDist)\n",
    "results(euc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7087bd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.885438689217759\n",
      "P@10: 0.7075184989429175\n",
      "Tri:  tensor(0.6804)\n",
      "Sil:  -0.12094105926059776\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 2-gram\n",
    "\n",
    "euc2_dis = matrix_calc(logVar[\"2-gram\"],euclidDist)\n",
    "results(euc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4efd5813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.6762024312896405\n",
      "P@10: 0.5240420190274842\n",
      "Tri:  tensor(0.6136)\n",
      "Sil:  -0.14871501783104946\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on 3-gram\n",
    "\n",
    "euc3_dis = matrix_calc(logVar[\"3-gram\"],euclidDist)\n",
    "results(euc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "361e5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9427193446088795\n",
      "P@10: 0.8232756342494715\n",
      "Tri:  tensor(0.7365)\n",
      "Sil:  -0.08381334058999264\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_euc = euc1_dis + euc2_dis\n",
    "results(agg_euc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef19c",
   "metadata": {},
   "source": [
    "### Jaccard based on 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "feb39b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance based on activity type\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1, s2 = set(list1), set(list2)\n",
    "    return 1 - len(s1 & s2) / len(s1 | s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "916e7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9655787526427062\n",
      "P@10: 0.9051268498942917\n",
      "Tri:  tensor(0.8467)\n",
      "Sil:  0.15975306042991522\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 1-gram\n",
    "Jacc1_dis = matrix_calc(logVar[\"charList\"],jaccard_similarity)\n",
    "results(Jacc1_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12164c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9827563424947146\n",
      "P@10: 0.9583047040169133\n",
      "Tri:  tensor(0.9151)\n",
      "Sil:  0.11957225318299027\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 2-gram\n",
    "Jacc2_dis = matrix_calc(logVar[\"dfList\"],jaccard_similarity)\n",
    "results(Jacc2_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58612249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9857293868921776\n",
      "P@10: 0.9660280126849894\n",
      "Tri:  tensor(0.9193)\n",
      "Sil:  0.05107219772286129\n"
     ]
    }
   ],
   "source": [
    "#Jaccard based on 3-gram\n",
    "Jacc3_dis = matrix_calc(logVar[\"dfList2\"],jaccard_similarity)\n",
    "results(Jacc3_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ab92a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9886363636363636\n",
      "P@10: 0.9360002642706132\n",
      "Tri:  tensor(0.8779)\n",
      "Sil:  0.15678760609316028\n"
     ]
    }
   ],
   "source": [
    "#Aggregation\n",
    "\n",
    "agg_jacc1 = Jacc1_dis + Jacc2_dis\n",
    "results(agg_jacc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c759b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9885702959830867\n",
      "P@10: 0.9520084566596195\n",
      "Tri:  tensor(0.8912)\n",
      "Sil:  0.1305832957211289\n"
     ]
    }
   ],
   "source": [
    "agg_jacc2 = Jacc1_dis + Jacc2_dis + Jacc3_dis\n",
    "results(agg_jacc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607bdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e67795",
   "metadata": {},
   "source": [
    "## Graph based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now consider edge types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26f94a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intEncoder(character_List):\n",
    "    return [np.where(np.array(list(dict.fromkeys(character_List)))==e)[0][0]for e in character_List]\n",
    "\n",
    "logVar[\"intList\"] = logVar[\"activity\"].apply(lambda x: intEncoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26a1b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. transfer intList to int_tupleList\n",
    "\n",
    "#Create tuple lists\n",
    "def tuple_list(list_of_encodedActivities):\n",
    "    #list.insert(0, '*')\n",
    "    #list.append('*')\n",
    "    list_new = []\n",
    "    last_element = list_of_encodedActivities[-1]\n",
    "    for i in range(len(list_of_encodedActivities)):\n",
    "        new = tuple(list_of_encodedActivities[i:i+2])\n",
    "        list_new.append(new)\n",
    "    del list_new[-1]\n",
    "    if list_of_encodedActivities.count(last_element) == 1: #check wether last activity in trace has some adjancency relation\n",
    "        list_new.append((last_element,)) ### NOT Correct\n",
    "    return list_new\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#tuple_list(q)\n",
    "\n",
    "logVar[\"int_tupleList\"] = logVar[\"intList\"].apply(lambda x: tuple_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4db1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. generate Adjacency List\n",
    "\n",
    "def adj_list(list_of_tuples):\n",
    "    adj_list_new = {}\n",
    "    try:\n",
    "        for node1, node2 in list_of_tuples:\n",
    "            #print(node1, node2)\n",
    "            if node1 not in adj_list_new:\n",
    "                newlist = []\n",
    "                newlist.append(node2)\n",
    "                adj_list_new[node1] = newlist\n",
    "                #print(adj_list3)\n",
    "        \n",
    "            else:\n",
    "                if node2 not in adj_list_new[node1]:\n",
    "                    #mylist.extend(adj_list3[node1])\n",
    "                    adj_list_new[node1].append(node2)\n",
    "                    #print(adj_list3)\n",
    "                    #adj_list3[node1] = mylist\n",
    "    \n",
    "    #in case activity has no adjacent activity - only possible for last activity --> tuple: (lastAct,)\n",
    "    except ValueError as ve:\n",
    "        lastValue = list_of_tuples[-1][0] \n",
    "        adj_list_new[lastValue] = list()\n",
    "    return list(adj_list_new.values())\n",
    "\n",
    "#q = [0,0,0,0,1,1,2,3,4,5,3,2,4,0,5,6]\n",
    "#l = tuple_list(q)\n",
    "#adj_list(l)\n",
    "\n",
    "logVar[\"int_adjList\"] = logVar[\"int_tupleList\"].apply(lambda x: adj_list(x))\n",
    "#logVar[\"int_adjList\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ad9bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now consider length\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs_4(graph, start, end):\n",
    "    \n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    #print(start, end)\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    while queue:\n",
    "        #print(queue)\n",
    "        node, distance = queue.popleft()\n",
    "        #if not node:\n",
    "            #print(start, end, queue)\n",
    "            #print(\"GRAPH LIST\", graph)\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end:\n",
    "            return distance \n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "        \n",
    "#x = {0: [0, 1], 1: [2, 1, 0, int], 2:[2], [3: [1, 5, 3, 7], 4: [3], 5: [6, 5], 6: [1, 7], 7: [8, 9, 7], 8: [5, 8, 10], 9: [3]}\n",
    "#y = [[0, 1, 5], [1, 2], [3, 4], [4, 2], [5, 0], [3, 6], []]\n",
    "#bfs_4(y, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8cbbc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import copy\n",
    "\n",
    "def reverse_graph(graph):\n",
    "    reversed_graph = defaultdict(list)\n",
    "    for node in graph:\n",
    "        for neighbor in graph[node]:\n",
    "            reversed_graph[neighbor].append(node)\n",
    "    return reversed_graph\n",
    "\n",
    "\n",
    "def bfs_5(graph, start, end):\n",
    "    queue = deque([(start, 0)])\n",
    "    seen = set()\n",
    "    visited = {}\n",
    "    while queue:\n",
    "        node, distance = queue.popleft()\n",
    "        if node in seen:\n",
    "            continue\n",
    "        seen.add(node)\n",
    "        if node == end: # maybe quicker if adjacent directly checked\n",
    "            return visited\n",
    "        for adjacent in graph.get(node, []):\n",
    "            queue.append((adjacent, distance + 1))\n",
    "            if adjacent not in visited:\n",
    "                visited.update({adjacent:distance})\n",
    "\n",
    "            \n",
    "def common_ancestors(graph, node1, node2): \n",
    "    #remove cross type edge between node1 and node2\n",
    "    graph = copy.deepcopy(graph)\n",
    "    graph[node1].remove(node2)\n",
    "    graph = {v: k for v, k in enumerate(graph)}\n",
    "    graphReverse = reverse_graph(graph)\n",
    "    setNode1 = bfs_5(graphReverse, node1, 0)\n",
    "    setNode2 = bfs_5(graphReverse, node2, 0)\n",
    "    if next((a for a in list(setNode1) if a in list(setNode2)), None) == None:\n",
    "        firstCommonAnces = next((a for a in list(setNode2) if a in list(setNode1)), None)\n",
    "    else:\n",
    "        firstCommonAnces = next((a for a in list(setNode1) if a in list(setNode2)), 0)\n",
    "    \n",
    "    #uses a hash map to identify the first common ancestor in both lists\n",
    "    #looks for the first common ancestor in setNode1, which can also be found in setNode2 \n",
    "    #--> this might not be the closest distance between setNode1 and setNode2\n",
    "    #--> e.g., for x= [0,1,3,7,5,6] and y= [4,5,7,8,3] 7 might be closest ancestor, although algo detects 3 !\n",
    "    #distance = setNode1[firstCommonAnces] + setNode2[firstCommonAnces]\n",
    "    \n",
    "    \n",
    "    if firstCommonAnces != None:  \n",
    "        ancesDistNode1 =  setNode1[firstCommonAnces] + 1 #the edge from node1 to first parent is counted as 0 by algorithm, therefore +1\n",
    "        ancesDistNode2 =  setNode2[firstCommonAnces] + 1\n",
    "        numberSkips = abs(ancesDistNode1 - ancesDistNode2)\n",
    "        numberCross = min(ancesDistNode1, ancesDistNode2)\n",
    "    else:\n",
    "        numberSkips, numberCross = (0,1)\n",
    "    return numberSkips, numberCross\n",
    "    #if all(x in crossType for x in i):\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#graphList = [[1], [2, 4, 1], [3, 2, 1], [], [5, 4], [5, 4, 6], [7], []]\n",
    "#c = [[1, 4], [2], [3], [0, 5], [3, 5], []]\n",
    "#c2 = {v: k for v, k in enumerate(c)}\n",
    "#common_ancestors(c, 4, 5)\n",
    "#reverse_graph(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "20f4dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List for decoding traces\n",
    "from collections import OrderedDict\n",
    "logVar[\"indexList\"] = logVar[\"activity\"].apply(lambda x: list(OrderedDict.fromkeys(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c6ba6",
   "metadata": {},
   "source": [
    "### Cosine Edge Type + length (no df relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "84f897bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph1:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = [['sequ', 1]]\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return np.array(self.structural_array)\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                self.structural_array[0][1] += 0\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append(['1back ',1])\n",
    "                    #self.structural_array.append(['back ',1])\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append(['2back ',2])\n",
    "                    #self.structural_array.append(['back ',2])\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append([str(x+1)+'back ',x+1])\n",
    "                    #self.structural_array.append(['back ',x+1])\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append([str(y-1)+'forward ',y-1])\n",
    "\n",
    "            else:\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                self.structural_array.append([str(numberCross)+'cross ',numberCross])\n",
    "  \n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b7903ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def transform_list_of_pairs(pairs):\n",
    "    return [pair[0] for pair in pairs]\n",
    "\n",
    "\n",
    "\n",
    "def count_entries(input_list):\n",
    "    # Count the occurrences of each unique entry in the list\n",
    "    counter = Counter(input_list)\n",
    "    \n",
    "    # Create a NumPy array from the counter dictionary\n",
    "    result = np.array([[key, count] for key, count in counter.items()], dtype=object)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "#input_list = ['sequ', '2back', '2back']\n",
    "#result = count_entries(input_list)\n",
    "#print(result)\n",
    "\n",
    "logVar[\"int_strucLengthList2\"] = logVar.apply(lambda x: Graph1(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "logVar[\"relFrequVec1\"] = logVar[\"int_strucLengthList2\"].apply(lambda x: transform_list_of_pairs(x))\n",
    "logVar[\"relFrequVec1\"] = logVar[\"relFrequVec1\"].apply(lambda x: count_entries(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fdd3a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.17197410147991543\n",
      "P@10: 0.15701638477801266\n",
      "Tri:  tensor(0.5332)\n",
      "Sil:  -0.13404679651077211\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "cos_graph_dis = matrix_calc(logVar[\"relFrequVec1\"],cosineDist)\n",
    "results(cos_graph_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "799bfe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.898090644820296\n",
      "Tri:  tensor(0.8625)\n",
      "Sil:  0.15896867249184918\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on edge types\n",
    "agg_cos = cos1_dis + cos2_dis + cos_graph_dis\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534419f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3075f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c470016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb5e8732",
   "metadata": {},
   "source": [
    "### Jaccard Edge Type and length + df relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a2b71f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph2:\n",
    "    # instance variables\n",
    "    def __init__(self, graph_list2, indexList):\n",
    "        # v is the number of nodes/vertices\n",
    "        self.time = 0\n",
    "        self.traversal_array = []\n",
    "        self.structural_array = []\n",
    "        #self.structural_array = []\n",
    "        self.graph_list = graph_list2\n",
    "        self.v = len(graph_list2)\n",
    "        self.indexList = indexList\n",
    "\n",
    "    # function for dfs\n",
    "    def dfs(self):\n",
    "        self.start_time = [-1]*self.v\n",
    "        self.end_time = [-1]*self.v\n",
    " \n",
    "        for node in range(self.v):\n",
    "            if self.start_time[node] == -1:\n",
    "                self.traverse_dfs(node)\n",
    "                \n",
    "        return self.structural_array\n",
    "        #print()\n",
    "        #print(\"DFS Traversal: \", self.traversal_array)\n",
    "        #print()\n",
    " \n",
    "    def traverse_dfs(self, node):\n",
    "        self.traversal_array.append(node)\n",
    "        # get the starting time\n",
    "        self.start_time[node] = self.time\n",
    "        self.time += 1\n",
    "        # traverse through the neighbours\n",
    "        for neighbour in self.graph_list[node]:\n",
    "\n",
    "            # when the neighbor was not yet visited\n",
    "            if self.start_time[neighbour] == -1:                \n",
    "                #self.structural_array[0][1] += 0\n",
    "                #self.structural_array.append('tree ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ')\n",
    "                self.structural_array.append('tree')\n",
    "                self.traverse_dfs(neighbour)\n",
    "                \n",
    "            # otherwise when the neighbour's visit is still ongoing:\n",
    "            elif self.end_time[neighbour] == -1:\n",
    "                if node == neighbour:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(1))\n",
    "                    #self.structural_array.append([str(1)+'b'])\n",
    "                \n",
    "                elif node in self.graph_list[neighbour]:\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(2))\n",
    "                    #self.structural_array.append(str(2)+'b')\n",
    "                    \n",
    "                else:\n",
    "                    x = bfs_4(self.graph_list, neighbour, node)\n",
    "                    self.structural_array.append('back ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(x+1))\n",
    "                    #self.structural_array.append(str(x+1)+'b')\n",
    "                \n",
    "            # otherwise when the neighbour's visit started before the current node's visit:\n",
    "            elif self.start_time[node] < self.start_time[neighbour]:\n",
    "                graph_list_copy = copy.deepcopy(self.graph_list)\n",
    "                graph_list_copy[node].remove(neighbour)\n",
    "                y = bfs_4(graph_list_copy, node, neighbour)\n",
    "                self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(y-1))\n",
    "                #self.structural_array.extend((y-1)*['forward']) # -1 to exclude one edge: (A:B,C;B:C;C:[]) ...the dist A --> C is 2 without forward edge, but we are skipping only one activity\n",
    "                #self.structural_array.append(str(y-1)+'f')\n",
    "                \n",
    "            else:\n",
    "                #Possibly first check, whether two nodes connected by cross-type have identical parent\n",
    "                numberSkips, numberCross = common_ancestors(self.graph_list, node, neighbour)\n",
    "                #self.structural_array.append('forward ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberSkips))\n",
    "                self.structural_array.append('cross ' + str(self.indexList[node]) + ' ' + str(self.indexList[neighbour]) + ' ' + str(numberCross))\n",
    "                #self.structural_array.append(str(numberSkips)+'f')\n",
    "                #self.structural_array.append(str(numberCross)+'c')\n",
    "    \n",
    "        # Indentation corrected:\n",
    "        self.end_time[node] = self.time\n",
    "        self.time += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3783713",
   "metadata": {},
   "outputs": [],
   "source": [
    "logVar[\"int_strucLengthList3\"] = logVar.apply(lambda x: Graph2(x.int_adjList, x.indexList).dfs(), axis =1)\n",
    "#logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab386336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.960029069767442\n",
      "P@10: 0.9242798625792813\n",
      "Tri:  tensor(0.9026)\n",
      "Sil:  0.08512902120313036\n"
     ]
    }
   ],
   "source": [
    "#Jacc similarity based on edge types\n",
    "\n",
    "jacc_graph = matrix_calc(logVar[\"int_strucLengthList3\"],jaccard_similarity)\n",
    "results(jacc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e122ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.898090644820296\n",
      "Tri:  tensor(0.8625)\n",
      "Sil:  0.15896867249184918\n"
     ]
    }
   ],
   "source": [
    "#Jacc sim based on edge types\n",
    "agg_jacc = Jacc1_dis + Jacc2_dis + jacc_graph\n",
    "results(agg_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67318150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308eff76",
   "metadata": {},
   "source": [
    "## Eventually Follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22dc753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial distance between strings\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def distanceSpatial(traceString, char1, char2):\n",
    "    positions_letter1 = [pos for pos, char in enumerate(traceString) if char == char1]\n",
    "    positions_letter2 = [pos for pos, char in enumerate(traceString) if char == char2]\n",
    "    \n",
    "    distList = []\n",
    "    \n",
    "\n",
    "    for i in range(len(positions_letter1)):\n",
    "        for j in range(len(positions_letter2)):\n",
    "            dist = positions_letter2[j] - positions_letter1[i]\n",
    "            if dist > 0:\n",
    "                    #print(dist)\n",
    "                distList.append(dist)\n",
    "                    \n",
    "    \n",
    "    if not distList: #distList.append(0) #in the case the char1 is after char2 asign dist 0, i.e. char2 cannot be reached from char1\n",
    "        return 0\n",
    "    else:\n",
    "        return 1/min(distList)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def commonDistance(trace1, trace2):\n",
    "    \n",
    "    commonSet = set(trace1) & set(trace2)\n",
    "\n",
    "    commonList = list(commonSet)\n",
    "    commonList.sort()\n",
    "    #print(commonList)\n",
    "\n",
    "    n = len(commonSet)\n",
    "    dist_matrix1 = np.zeros((n,n))\n",
    "    dist_matrix2 = np.zeros((n,n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix1[i,j] = distanceSpatial(trace1, commonList[i], commonList[j])\n",
    "        \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dist_matrix2[i,j] = distanceSpatial(trace2, commonList[i], commonList[j])\n",
    "    \n",
    "    #print(dist_matrix1, dist_matrix2)\n",
    "    return distance.cosine(dist_matrix1.ravel(), dist_matrix2.ravel())\n",
    "\n",
    "\n",
    "\n",
    "#x = 'ABCDEF'\n",
    "#y = 'ABCDEBCDEBCDEF'\n",
    "#z = 'ABCDEBCDEF'\n",
    "#print(dist_matrix)\n",
    "#distanceSpatial(x, 'A', 'E')\n",
    "#listVec = logVar[\"strings\"]\n",
    "#x= listVec[0]\n",
    "#y= listVec[1]\n",
    "#commonDistance(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e77fecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.46128435517970406\n",
      "P@10: 0.4549550739957716\n",
      "Tri:  tensor(0.8286)\n",
      "Sil:  0.06766086823284875\n"
     ]
    }
   ],
   "source": [
    "dist_matrix_evFollows = matrix_calc(logVar[\"strings\"],commonDistance)\n",
    "agg_evFollows = 0.7*dist_matrix_evFollows + 0.3*cos1_dis \n",
    "results(agg_evFollows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f9fa8",
   "metadata": {},
   "source": [
    "## Maximal Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82a82836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from suffix_tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3ba0fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a a', 'b', 'b d', 'c', 'c b', 'c d c', 'd', 'd b', 'd c', 'e']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tree.maximal_repeats\n",
    "def maxRepeat(tree):\n",
    "    mrList=[]\n",
    "    for C, path in sorted(tree.maximal_repeats()):\n",
    "        mrList.append(str(path))\n",
    "    return mrList\n",
    "\n",
    "#test_tree = Tree({\"A\": \"aaacdcdcbedbccbadbdebdc\"})\n",
    "#maxRepeat(test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e6fa179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create vector based on maximal repeats\n",
    "logVar[\"mrList\"] = logVar[\"strings\"].apply(lambda x: maxRepeat(Tree({\"A\": x})))\n",
    "logVar[\"mrVector\"] = logVar[\"mrList\"].apply(lambda x: createVector(tuple(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75d75b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9571220930232558\n",
      "P@10: 0.9028409090909092\n",
      "Tri:  tensor(0.8912)\n",
      "Sil:  0.18629615358356524\n"
     ]
    }
   ],
   "source": [
    "#Cosine distance based on maxR\n",
    "\n",
    "cos_mr = matrix_calc(logVar[\"mrVector\"],cosineDist)\n",
    "results(cos_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3991e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.7854122621564482\n",
      "P@10: 0.6207650634249472\n",
      "Tri:  tensor(0.6617)\n",
      "Sil:  -0.06668386684998291\n"
     ]
    }
   ],
   "source": [
    "#Euclidean distance based on maxR\n",
    "\n",
    "euc_mr = matrix_calc(logVar[\"mrVector\"],euclidDist)\n",
    "results(euc_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f51a3b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:   0.9541490486257929\n",
      "P@10: 0.9013940274841438\n",
      "Tri:  tensor(0.8847)\n",
      "Sil:  0.1362689156049775\n"
     ]
    }
   ],
   "source": [
    "#Jaccard similarity based on maxR\n",
    "\n",
    "jacc_mr = matrix_calc(logVar[\"mrList\"],jaccard_similarity)\n",
    "results(jacc_mr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fba8d",
   "metadata": {},
   "source": [
    "## Optimal Alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49788bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "import math\n",
    "\n",
    "\n",
    "# Define sequences\n",
    "#seq1 = \"ACCGTTTTT\"\n",
    "#seq2 = \"ACG\"\n",
    "\n",
    "# Perform global alignment with scoring:\n",
    "# match = +2, mismatch = -1, gap open = -2, gap extend = 0\n",
    "#alignments = pairwise2.align.globalms(seq1, seq2, 2, -1, -2, 0)\n",
    "\n",
    "#alignments\n",
    "#score --> alignments[0][2]\n",
    "\n",
    "def optAlign1(string1, string2):\n",
    "    alignments = pairwise2.align.globalms(string1, string2, 1, 0, 0, 0)    \n",
    "    return 1 - (alignments[0][2]/max(len(string1),len(string2)))\n",
    "\n",
    "\n",
    "def optAlign2(string1, string2):\n",
    "    alignments = pairwise2.align.globalms(string1, string2, 2, -1, -2, 0)    \n",
    "    return 1 / (1 + math.exp(0.05*alignments[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19308a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i, n):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#print(listVec[i], listVec[j])\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     dist_matrix[i,j] \u001b[38;5;241m=\u001b[39m \u001b[43moptAlign1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlistVec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlistVec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#print(dist_matrix[i,j])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     dist_matrix[j,i] \u001b[38;5;241m=\u001b[39m dist_matrix[i,j]       \u001b[38;5;66;03m# for the symmetric part, no computation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[66], line 17\u001b[0m, in \u001b[0;36moptAlign1\u001b[1;34m(string1, string2)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptAlign1\u001b[39m(string1, string2):\n\u001b[1;32m---> 17\u001b[0m     alignments \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobalms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (alignments[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(string1),\u001b[38;5;28mlen\u001b[39m(string2)))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:493\u001b[0m, in \u001b[0;36malign.alignment_function.__call__\u001b[1;34m(self, *args, **keywds)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m\"\"\"Call the alignment instance already created.\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m keywds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywds)\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _align(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywds)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:597\u001b[0m, in \u001b[0;36m_align\u001b[1;34m(sequenceA, sequenceB, match_fn, gap_A_fn, gap_B_fn, penalize_extend_when_opening, penalize_end_gaps, align_globally, gap_char, force_generic, score_only, one_alignment_only)\u001b[0m\n\u001b[0;32m    594\u001b[0m starts \u001b[38;5;241m=\u001b[39m _find_start(score_matrix, best_score, align_globally)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Recover the alignments and return them.\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m alignments \u001b[38;5;241m=\u001b[39m \u001b[43m_recover_alignments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequenceA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequenceB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign_globally\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_alignment_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_A_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_B_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m alignments:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;66;03m# This may happen, see recover_alignments for explanation\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     score_matrix, trace_matrix \u001b[38;5;241m=\u001b[39m _reverse_matrices(score_matrix, trace_matrix)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:1051\u001b[0m, in \u001b[0;36m_recover_alignments\u001b[1;34m(sequenceA, sequenceB, starts, best_score, score_matrix, trace_matrix, align_globally, gap_char, one_alignment_only, gap_A_fn, gap_B_fn, reverse)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     trace \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m   1050\u001b[0m     col_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43m_find_gap_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequenceA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequenceB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mali_seqA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mali_seqB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_gap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgap_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrace_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgap_B_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign_globally\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m     ali_seqA, ali_seqB, row, col, in_process, dead_end \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace:  \u001b[38;5;66;03m# There is another path to follow...\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:1214\u001b[0m, in \u001b[0;36m_find_gap_open\u001b[1;34m(sequenceA, sequenceB, ali_seqA, ali_seqB, end, row, col, col_gap, gap_char, score_matrix, trace_matrix, in_process, gap_fn, target, index, direction, best_score, align_globally)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     row \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1213\u001b[0m     ali_seqA \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sequenceA[row : row \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1214\u001b[0m     ali_seqB \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gap_char\n\u001b[0;32m   1215\u001b[0m actual_score \u001b[38;5;241m=\u001b[39m score_matrix[row][col] \u001b[38;5;241m+\u001b[39m gap_fn(index, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m align_globally \u001b[38;5;129;01mand\u001b[39;00m score_matrix[row][col] \u001b[38;5;241m==\u001b[39m best_score:\n\u001b[0;32m   1217\u001b[0m     \u001b[38;5;66;03m# We have run through a 'zero-score' extension and discard it\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Calculate cosine similarity based on Spatial distance\n",
    "\n",
    "listVec = logVar[\"strings\"]\n",
    "\n",
    "n = len(listVec)\n",
    "dist_matrix = np.zeros((n,n))    # initialize distance matrix to a square of zeros\n",
    "\n",
    "for i in range(n):\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    for j in range(i, n):\n",
    "        #print(listVec[i], listVec[j])\n",
    "        dist_matrix[i,j] = optAlign1(listVec[i], listVec[j])\n",
    "        #print(dist_matrix[i,j])\n",
    "        dist_matrix[j,i] = dist_matrix[i,j]       # for the symmetric part, no computation\n",
    "        \n",
    "Align_dis1 = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a0d0ca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Align_dis1 \u001b[38;5;241m=\u001b[39m \u001b[43mmatrix_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogVar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptAlign1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m results(Align_dis1)\n",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m, in \u001b[0;36mmatrix_calc\u001b[1;34m(features, distance)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i, n):\n\u001b[1;32m----> 7\u001b[0m         dist_matrix[i,j] \u001b[38;5;241m=\u001b[39m \u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m         dist_matrix[j,i] \u001b[38;5;241m=\u001b[39m dist_matrix[i,j]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist_matrix\n",
      "Cell \u001b[1;32mIn[66], line 17\u001b[0m, in \u001b[0;36moptAlign1\u001b[1;34m(string1, string2)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptAlign1\u001b[39m(string1, string2):\n\u001b[1;32m---> 17\u001b[0m     alignments \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobalms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (alignments[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(string1),\u001b[38;5;28mlen\u001b[39m(string2)))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:493\u001b[0m, in \u001b[0;36malign.alignment_function.__call__\u001b[1;34m(self, *args, **keywds)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m\"\"\"Call the alignment instance already created.\"\"\"\u001b[39;00m\n\u001b[0;32m    492\u001b[0m keywds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywds)\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _align(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywds)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:597\u001b[0m, in \u001b[0;36m_align\u001b[1;34m(sequenceA, sequenceB, match_fn, gap_A_fn, gap_B_fn, penalize_extend_when_opening, penalize_end_gaps, align_globally, gap_char, force_generic, score_only, one_alignment_only)\u001b[0m\n\u001b[0;32m    594\u001b[0m starts \u001b[38;5;241m=\u001b[39m _find_start(score_matrix, best_score, align_globally)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Recover the alignments and return them.\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m alignments \u001b[38;5;241m=\u001b[39m \u001b[43m_recover_alignments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequenceA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequenceB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrace_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign_globally\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_alignment_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_A_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgap_B_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m alignments:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;66;03m# This may happen, see recover_alignments for explanation\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     score_matrix, trace_matrix \u001b[38;5;241m=\u001b[39m _reverse_matrices(score_matrix, trace_matrix)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\pm4py\\lib\\site-packages\\Bio\\pairwise2.py:967\u001b[0m, in \u001b[0;36m_recover_alignments\u001b[1;34m(sequenceA, sequenceB, starts, best_score, score_matrix, trace_matrix, align_globally, gap_char, one_alignment_only, gap_A_fn, gap_B_fn, reverse)\u001b[0m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[0;32m    964\u001b[0m     in_process \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    965\u001b[0m         (ali_seqA, ali_seqB, end, row, col, \u001b[38;5;28;01mFalse\u001b[39;00m, trace_matrix[row][col])\n\u001b[0;32m    966\u001b[0m     ]\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m in_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m(tracebacks) \u001b[38;5;241m<\u001b[39m MAX_ALIGNMENTS:\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Although we allow a gap in seqB to be followed by a gap in seqA,\u001b[39;00m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# we don't want to allow it the other way round, since this would\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;66;03m# give redundant alignments of type: A-  vs.  -A\u001b[39;00m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;66;03m#                                    -B       B-\u001b[39;00m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;66;03m# Thus we need to keep track if a gap in seqA was opened (col_gap)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# and stop the backtrace (dead_end) if a gap in seqB follows.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;66;03m# Attention: This may fail, if the gap-penalties for both strands are\u001b[39;00m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# different. In this case the second alignment may be the only optimal\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# alignment. Thus it can happen that no alignment is returned. For\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# this case a workaround was implemented, which reverses the input and\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# the matrices (this happens in _reverse_matrices) and repeats the\u001b[39;00m\n\u001b[0;32m    980\u001b[0m     \u001b[38;5;66;03m# backtrace. The variable 'reverse' keeps track of this.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m     dead_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     ali_seqA, ali_seqB, end, row, col, col_gap, trace \u001b[38;5;241m=\u001b[39m in_process\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Align_dis1 = matrix_calc(logVar[\"strings\"],optAlign1)\n",
    "results(Align_dis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Align_dis2 = matrix_calc(logVar[\"strings\"],optAlign2)\n",
    "results(Align_dis2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
